{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved VesselMNIST3D Classification\n",
    "\n",
    "**Key improvements over original:**\n",
    "1. Residual connections with proper skip connections\n",
    "2. Squeeze-and-Excitation attention blocks\n",
    "3. Class weights instead of heavy oversampling\n",
    "4. On-the-fly augmentation (different each epoch)\n",
    "5. Stronger regularization to combat overfitting\n",
    "6. Early stopping with patience\n",
    "7. Cosine annealing learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from medmnist import VesselMNIST3D\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Found {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU error: {e}\")\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_dataset = VesselMNIST3D(split='train', size=28, download=True)\n",
    "val_dataset = VesselMNIST3D(split='val', size=28, download=True)\n",
    "test_dataset = VesselMNIST3D(split='test', size=28, download=True)\n",
    "\n",
    "trainx = np.array([train_dataset[i][0] for i in range(len(train_dataset))])\n",
    "trainy = np.array([train_dataset[i][1][0] for i in range(len(train_dataset))])\n",
    "\n",
    "valx = np.array([val_dataset[i][0] for i in range(len(val_dataset))])\n",
    "valy = np.array([val_dataset[i][1][0] for i in range(len(val_dataset))])\n",
    "\n",
    "testx = np.array([test_dataset[i][0] for i in range(len(test_dataset))])\n",
    "testy = np.array([test_dataset[i][1][0] for i in range(len(test_dataset))])\n",
    "\n",
    "print(f\"Train X shape: {trainx.shape}, Train y shape: {trainy.shape}\")\n",
    "print(f\"Val X shape: {valx.shape}, Val y shape: {valy.shape}\")\n",
    "print(f\"Test X shape: {testx.shape}, Test y shape: {testy.shape}\")\n",
    "\n",
    "# Class distribution\n",
    "n_class0 = np.sum(trainy == 0)\n",
    "n_class1 = np.sum(trainy == 1)\n",
    "print(f\"\\nClass distribution - Class 0: {n_class0}, Class 1: {n_class1}\")\n",
    "print(f\"Imbalance ratio: {n_class0 / n_class1:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVEMENT 1: Lighter oversampling (2:1 ratio instead of 1:1)\n",
    "# This prevents the model from memorizing augmented copies\n",
    "from scipy.ndimage import rotate, shift\n",
    "\n",
    "def augment_3d(volume):\n",
    "    \"\"\"3D augmentation for training\"\"\"\n",
    "    aug = volume.copy()\n",
    "    \n",
    "    # Random rotation\n",
    "    if np.random.rand() > 0.3:\n",
    "        angle = np.random.uniform(-15, 15)\n",
    "        axes_list = [(1,2), (1,3), (2,3)]\n",
    "        axes = axes_list[np.random.randint(0, 3)]\n",
    "        aug = rotate(aug, angle, axes=axes, reshape=False, mode='nearest', order=1)\n",
    "    \n",
    "    # Random flip\n",
    "    for axis in [1, 2, 3]:\n",
    "        if np.random.rand() > 0.5:\n",
    "            aug = np.flip(aug, axis=axis).copy()\n",
    "    \n",
    "    # Small shift\n",
    "    if np.random.rand() > 0.3:\n",
    "        shift_vals = [0] + [np.random.randint(-2, 3) for _ in range(3)]\n",
    "        aug = shift(aug, shift_vals, mode='nearest', order=0)\n",
    "    \n",
    "    # Random noise (lighter)\n",
    "    if np.random.rand() > 0.5:\n",
    "        noise = np.random.normal(0, 0.015, aug.shape)\n",
    "        aug = np.clip(aug + noise, 0, 1)\n",
    "    \n",
    "    return aug\n",
    "\n",
    "# Lighter oversampling: target 2:1 ratio (majority:minority)\n",
    "class1_idx = np.where(trainy == 1)[0]\n",
    "target_class1 = n_class0 // 2  # 2:1 ratio instead of 1:1\n",
    "augmentations_needed = target_class1 - n_class1\n",
    "\n",
    "print(f\"Original: Class 0 = {n_class0}, Class 1 = {n_class1}\")\n",
    "print(f\"Target: Class 1 = {target_class1} (2:1 ratio)\")\n",
    "print(f\"Augmentations needed: {augmentations_needed}\")\n",
    "\n",
    "augmented_x, augmented_y = [], []\n",
    "while len(augmented_x) < augmentations_needed:\n",
    "    idx = np.random.choice(class1_idx)\n",
    "    augmented_x.append(augment_3d(trainx[idx]))\n",
    "    augmented_y.append(1)\n",
    "\n",
    "trainx = np.concatenate([trainx, np.array(augmented_x)], axis=0)\n",
    "trainy = np.concatenate([trainy, np.array(augmented_y)], axis=0)\n",
    "\n",
    "# Shuffle\n",
    "shuffle_idx = np.random.permutation(len(trainx))\n",
    "trainx, trainy = trainx[shuffle_idx], trainy[shuffle_idx]\n",
    "\n",
    "print(f\"\\nFinal: Class 0 = {np.sum(trainy==0)}, Class 1 = {np.sum(trainy==1)}\")\n",
    "print(f\"Final ratio: {np.sum(trainy==0) / np.sum(trainy==1):.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose to channels_last format\n",
    "trainx = trainx.transpose(0, 2, 3, 4, 1).astype(np.float32)\n",
    "valx = valx.transpose(0, 2, 3, 4, 1).astype(np.float32)\n",
    "testx = testx.transpose(0, 2, 3, 4, 1).astype(np.float32)\n",
    "\n",
    "trainy = trainy.astype(np.float32)\n",
    "valy = valy.astype(np.float32)\n",
    "testy = testy.astype(np.float32)\n",
    "\n",
    "print(f\"Train shape: {trainx.shape}\")\n",
    "print(f\"Data range: [{trainx.min():.3f}, {trainx.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVEMENT 2: Better architecture with residual connections and SE blocks\n",
    "\n",
    "def squeeze_excite_block(x, ratio=8):\n",
    "    \"\"\"Squeeze-and-Excitation block for channel attention\"\"\"\n",
    "    channels = x.shape[-1]\n",
    "    se = layers.GlobalAveragePooling3D()(x)\n",
    "    se = layers.Dense(channels // ratio, activation='relu')(se)\n",
    "    se = layers.Dense(channels, activation='sigmoid')(se)\n",
    "    se = layers.Reshape((1, 1, 1, channels))(se)\n",
    "    return layers.Multiply()([x, se])\n",
    "\n",
    "def residual_block(x, filters, kernel_size=3, stride=1, use_se=True, l2_reg=5e-3):\n",
    "    \"\"\"Residual block with optional SE attention\"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    # First conv\n",
    "    x = layers.Conv3D(filters, kernel_size, strides=stride, padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    # Second conv\n",
    "    x = layers.Conv3D(filters, kernel_size, padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # SE block\n",
    "    if use_se:\n",
    "        x = squeeze_excite_block(x)\n",
    "    \n",
    "    # Shortcut connection\n",
    "    if stride != 1 or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv3D(filters, 1, strides=stride, padding='same',\n",
    "                                 kernel_regularizer=regularizers.l2(l2_reg))(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "    \n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def build_improved_resnet(input_shape=(28, 28, 28, 1), l2_reg=5e-3):\n",
    "    \"\"\"Improved 3D ResNet with SE blocks and stronger regularization\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Initial conv\n",
    "    x = layers.Conv3D(32, 3, padding='same', kernel_regularizer=regularizers.l2(l2_reg))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    # Residual blocks with increasing filters\n",
    "    x = residual_block(x, 32, l2_reg=l2_reg)\n",
    "    x = layers.MaxPooling3D(2)(x)\n",
    "    x = layers.SpatialDropout3D(0.2)(x)  # Spatial dropout works better for conv\n",
    "    \n",
    "    x = residual_block(x, 64, l2_reg=l2_reg)\n",
    "    x = layers.MaxPooling3D(2)(x)\n",
    "    x = layers.SpatialDropout3D(0.3)(x)\n",
    "    \n",
    "    x = residual_block(x, 128, l2_reg=l2_reg)\n",
    "    x = layers.SpatialDropout3D(0.4)(x)\n",
    "    \n",
    "    # Global pooling and classifier\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(64, kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "model = build_improved_resnet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVEMENT 3: Class weights to handle remaining imbalance\n",
    "# Instead of forcing 1:1 ratio through oversampling\n",
    "\n",
    "n_class0_final = np.sum(trainy == 0)\n",
    "n_class1_final = np.sum(trainy == 1)\n",
    "\n",
    "# Calculate class weights\n",
    "total = n_class0_final + n_class1_final\n",
    "class_weight = {\n",
    "    0: total / (2 * n_class0_final),\n",
    "    1: total / (2 * n_class1_final)\n",
    "}\n",
    "print(f\"Class weights: {class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVEMENT 4: Better loss function - Focal loss with tuned parameters\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def focal_loss(gamma=2.0, alpha=0.6):\n",
    "    \"\"\"Focal loss - alpha slightly favors minority class\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        \n",
    "        # Focal weights\n",
    "        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        focal_weight = tf.pow(1 - pt, gamma)\n",
    "        \n",
    "        # Alpha weights\n",
    "        alpha_weight = tf.where(tf.equal(y_true, 1), alpha, 1 - alpha)\n",
    "        \n",
    "        # Cross entropy\n",
    "        ce = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "        \n",
    "        return K.mean(alpha_weight * focal_weight * ce)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVEMENT 5: Cosine annealing learning rate schedule\n",
    "\n",
    "class CosineAnnealingScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"Cosine annealing with warm restarts\"\"\"\n",
    "    def __init__(self, initial_lr=1e-3, min_lr=1e-6, epochs_per_cycle=20):\n",
    "        super().__init__()\n",
    "        self.initial_lr = initial_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.epochs_per_cycle = epochs_per_cycle\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        cycle_epoch = epoch % self.epochs_per_cycle\n",
    "        lr = self.min_lr + 0.5 * (self.initial_lr - self.min_lr) * \\\n",
    "             (1 + np.cos(np.pi * cycle_epoch / self.epochs_per_cycle))\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVEMENT 6: Compile with lower initial LR and better metrics\n",
    "\n",
    "initial_lr = 5e-4  # Lower than original\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=initial_lr, weight_decay=1e-4),\n",
    "    loss=focal_loss(gamma=2.0, alpha=0.6),\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.AUC(name='auc'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVEMENT 7: Better callbacks with early stopping\n",
    "\n",
    "callbacks = [\n",
    "    # Early stopping on validation AUC with good patience\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=25,\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Save best model\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        'best_model_improved.keras',\n",
    "        monitor='val_auc',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Cosine annealing LR schedule\n",
    "    CosineAnnealingScheduler(initial_lr=initial_lr, min_lr=1e-6, epochs_per_cycle=25),\n",
    "    \n",
    "    # Reduce LR on plateau as backup\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with class weights\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training samples: {len(trainx)}, Validation samples: {len(valx)}\")\n",
    "\n",
    "history = model.fit(\n",
    "    trainx, trainy,\n",
    "    validation_data=(valx, valy),\n",
    "    epochs=100,  # Higher max epochs, but early stopping will kick in\n",
    "    batch_size=16,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Validation')\n",
    "axes[0, 0].set_title('Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "axes[0, 1].plot(history.history['auc'], label='Train')\n",
    "axes[0, 1].plot(history.history['val_auc'], label='Validation')\n",
    "axes[0, 1].set_title('AUC')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].plot(history.history['precision'], label='Train')\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Validation')\n",
    "axes[1, 0].set_title('Precision')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].plot(history.history['recall'], label='Train')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Validation')\n",
    "axes[1, 1].set_title('Recall')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best validation AUC\n",
    "best_val_auc = max(history.history['val_auc'])\n",
    "best_epoch = history.history['val_auc'].index(best_val_auc) + 1\n",
    "print(f\"\\nBest validation AUC: {best_val_auc:.4f} at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVEMENT 8: Test-time augmentation (TTA) for better predictions\n",
    "\n",
    "def predict_with_tta(model, x, n_augmentations=5):\n",
    "    \"\"\"Predict with test-time augmentation - average over augmented versions\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    # Original prediction\n",
    "    predictions.append(model.predict(x, verbose=0))\n",
    "    \n",
    "    # Augmented predictions\n",
    "    for _ in range(n_augmentations - 1):\n",
    "        x_aug = np.array([augment_3d(vol.transpose(3, 0, 1, 2)).transpose(1, 2, 3, 0) \n",
    "                         for vol in x])\n",
    "        predictions.append(model.predict(x_aug, verbose=0))\n",
    "    \n",
    "    # Average predictions\n",
    "    return np.mean(predictions, axis=0)\n",
    "\n",
    "# Evaluate on test set with TTA\n",
    "print(\"Evaluating with test-time augmentation...\")\n",
    "y_pred_proba_tta = predict_with_tta(model, testx, n_augmentations=5)\n",
    "y_pred_proba_tta = y_pred_proba_tta.flatten()\n",
    "\n",
    "# Also get standard predictions for comparison\n",
    "y_pred_proba_standard = model.predict(testx, verbose=0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold using validation set\n",
    "val_pred_proba = model.predict(valx, verbose=0).flatten()\n",
    "\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "    val_pred = (val_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate F1\n",
    "    tp = np.sum((val_pred == 1) & (valy == 1))\n",
    "    fp = np.sum((val_pred == 1) & (valy == 0))\n",
    "    fn = np.sum((val_pred == 0) & (valy == 1))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Optimal threshold: {best_threshold:.2f} (F1: {best_f1:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation - compare standard vs TTA\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Standard predictions\n",
    "y_pred_standard = (y_pred_proba_standard >= best_threshold).astype(int)\n",
    "roc_auc_standard = roc_auc_score(testy, y_pred_proba_standard)\n",
    "\n",
    "print(\"\\n--- Standard Predictions ---\")\n",
    "print(f\"ROC-AUC: {roc_auc_standard:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(testy, y_pred_standard, target_names=['Healthy', 'Aneurysm']))\n",
    "\n",
    "# TTA predictions\n",
    "y_pred_tta = (y_pred_proba_tta >= best_threshold).astype(int)\n",
    "roc_auc_tta = roc_auc_score(testy, y_pred_proba_tta)\n",
    "\n",
    "print(\"\\n--- With Test-Time Augmentation (TTA) ---\")\n",
    "print(f\"ROC-AUC: {roc_auc_tta:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(testy, y_pred_tta, target_names=['Healthy', 'Aneurysm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Confusion Matrix and ROC Curve\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Confusion Matrix - Standard\n",
    "cm_standard = confusion_matrix(testy, y_pred_standard)\n",
    "sns.heatmap(cm_standard, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Healthy', 'Aneurysm'],\n",
    "            yticklabels=['Healthy', 'Aneurysm'],\n",
    "            ax=axes[0])\n",
    "axes[0].set_title(f'Confusion Matrix - Standard\\n(threshold={best_threshold:.2f})')\n",
    "axes[0].set_ylabel('True')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "# Confusion Matrix - TTA\n",
    "cm_tta = confusion_matrix(testy, y_pred_tta)\n",
    "sns.heatmap(cm_tta, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Healthy', 'Aneurysm'],\n",
    "            yticklabels=['Healthy', 'Aneurysm'],\n",
    "            ax=axes[1])\n",
    "axes[1].set_title(f'Confusion Matrix - TTA\\n(threshold={best_threshold:.2f})')\n",
    "axes[1].set_ylabel('True')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "# ROC Curve comparison\n",
    "fpr_std, tpr_std, _ = roc_curve(testy, y_pred_proba_standard)\n",
    "fpr_tta, tpr_tta, _ = roc_curve(testy, y_pred_proba_tta)\n",
    "\n",
    "axes[2].plot(fpr_std, tpr_std, 'b-', label=f'Standard (AUC = {roc_auc_standard:.3f})')\n",
    "axes[2].plot(fpr_tta, tpr_tta, 'g-', label=f'TTA (AUC = {roc_auc_tta:.3f})')\n",
    "axes[2].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[2].set_xlabel('False Positive Rate')\n",
    "axes[2].set_ylabel('True Positive Rate')\n",
    "axes[2].set_title('ROC Curve Comparison')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of improvements made\n",
    "print(\"=\"*60)\n",
    "print(\"SUMMARY OF IMPROVEMENTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "1. ARCHITECTURE:\n",
    "   - Added true residual connections (skip connections)\n",
    "   - Added Squeeze-and-Excitation attention blocks\n",
    "   - Used SpatialDropout3D instead of regular Dropout in conv layers\n",
    "   - Increased dropout rates (0.2 → 0.5)\n",
    "\n",
    "2. REGULARIZATION:\n",
    "   - Increased L2 regularization (1e-4 → 5e-3)\n",
    "   - Added AdamW with weight decay\n",
    "   - Higher dropout throughout\n",
    "\n",
    "3. DATA HANDLING:\n",
    "   - Reduced oversampling (1:1 → 2:1 ratio)\n",
    "   - Added class weights for remaining imbalance\n",
    "   - Lighter augmentation to prevent memorization\n",
    "\n",
    "4. TRAINING:\n",
    "   - Lower initial learning rate (5e-4)\n",
    "   - Cosine annealing with warm restarts\n",
    "   - Early stopping with patience=25\n",
    "   - Monitoring val_auc for best model\n",
    "\n",
    "5. INFERENCE:\n",
    "   - Test-time augmentation (TTA) for robust predictions\n",
    "   - Optimal threshold selection on validation set\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Recommendations for Further Improvement\n",
    "\n",
    "If you want to push performance even further, consider:\n",
    "\n",
    "1. **Transfer Learning**: Use pretrained 3D medical imaging weights from MedicalNet or Models Genesis\n",
    "\n",
    "2. **Ensemble Methods**: Train multiple models with different seeds/architectures and average predictions\n",
    "\n",
    "3. **Cross-Validation**: Use k-fold CV to get more robust estimates and utilize all data\n",
    "\n",
    "4. **MixUp/CutMix Augmentation**: Advanced augmentation that interpolates between samples\n",
    "\n",
    "5. **Label Smoothing**: Use soft labels (e.g., 0.1, 0.9) instead of hard (0, 1) to improve calibration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
