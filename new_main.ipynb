{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and setup\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.widgets import Slider\n",
    "import matplotlib\n",
    "import matplotlib.font_manager\n",
    "\n",
    "from medmnist import VesselMNIST3D\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117fa961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load data\n",
    "train_dataset = VesselMNIST3D(split='train', size=28, download=True)\n",
    "trainx = []\n",
    "trainy = []\n",
    "\n",
    "test_dataset = VesselMNIST3D(split='test', size=28, download=True)\n",
    "testx = []\n",
    "testy = []\n",
    "\n",
    "val_dataset = VesselMNIST3D(split='val', size=28, download=True)\n",
    "valx = []\n",
    "valy = []\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    trainx.append(train_dataset[i][0])\n",
    "    trainy.append(train_dataset[i][1])\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    testx.append(test_dataset[i][0])\n",
    "    testy.append(test_dataset[i][1])\n",
    "\n",
    "for i in range(len(val_dataset)):\n",
    "    valx.append(val_dataset[i][0])\n",
    "    valy.append(val_dataset[i][1])\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Training samples: {len(trainx)}\")\n",
    "print(f\"Validation samples: {len(valx)}\")\n",
    "print(f\"Test samples: {len(testx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67789bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Analyze original class distribution\n",
    "train_labels = np.array(trainy).flatten()\n",
    "val_labels = np.array(valy).flatten()\n",
    "test_labels = np.array(testy).flatten()\n",
    "\n",
    "unique_train, counts_train = np.unique(train_labels, return_counts=True)\n",
    "unique_val, counts_val = np.unique(val_labels, return_counts=True)\n",
    "unique_test, counts_test = np.unique(test_labels, return_counts=True)\n",
    "\n",
    "print(f\"Training - Class 0 (Healthy): {counts_train[0]}, Class 1 (Aneurysm): {counts_train[1]}\")\n",
    "print(f\"Validation - Class 0 (Healthy): {counts_val[0]}, Class 1 (Aneurysm): {counts_val[1]}\")\n",
    "print(f\"Test - Class 0 (Healthy): {counts_test[0]}, Class 1 (Aneurysm): {counts_test[1]}\")\n",
    "print(f\"\\nClass imbalance ratio (train): {counts_train[1]/counts_train[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36153f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Augmentation functions and execution\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate, zoom, shift\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import tensorflow as tf\n",
    "\n",
    "def augment_3d_volume(volume, num_augmentations=5):\n",
    "    augmented_volumes = []\n",
    "    \n",
    "    if hasattr(volume, 'numpy'):\n",
    "        volume = volume.numpy()\n",
    "    \n",
    "    original_shape = volume.shape\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "        aug_volume = volume.copy()\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            angle = np.random.uniform(-15, 15)\n",
    "            axes_options = [(1, 2), (1, 3), (2, 3)]\n",
    "            axes = axes_options[np.random.randint(0, len(axes_options))]\n",
    "            aug_volume = rotate(aug_volume, angle, axes=axes, reshape=False, mode='nearest')\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            scale_factor = np.random.uniform(0.9, 1.1)\n",
    "            zoom_factors = [1, scale_factor, scale_factor, scale_factor]\n",
    "            aug_volume = zoom(aug_volume, zoom_factors, mode='nearest')\n",
    "            aug_volume = resize_to_original(aug_volume, original_shape)\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            axis = np.random.randint(1, 4)\n",
    "            aug_volume = np.flip(aug_volume, axis=axis)\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            shift_amount = [0] + [np.random.randint(-3, 4) for _ in range(3)]\n",
    "            aug_volume = shift(aug_volume, shift_amount, mode='nearest')\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            noise = np.random.normal(0, 0.01, aug_volume.shape)\n",
    "            aug_volume = aug_volume + noise\n",
    "            aug_volume = np.clip(aug_volume, 0, 1)\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            brightness_factor = np.random.uniform(0.9, 1.1)\n",
    "            aug_volume = aug_volume * brightness_factor\n",
    "            aug_volume = np.clip(aug_volume, 0, 1)\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            mean = np.mean(aug_volume)\n",
    "            aug_volume = (aug_volume - mean) * np.random.uniform(0.9, 1.1) + mean\n",
    "            aug_volume = np.clip(aug_volume, 0, 1)\n",
    "        \n",
    "        if np.random.rand() > 0.7:\n",
    "            aug_volume = elastic_transform_3d(aug_volume, alpha=5, sigma=3)\n",
    "        \n",
    "        augmented_volumes.append(aug_volume)\n",
    "    \n",
    "    return augmented_volumes\n",
    "\n",
    "def elastic_transform_3d(volume, alpha=10, sigma=3):\n",
    "    shape = volume.shape[1:]\n",
    "    \n",
    "    dx = np.random.randn(*shape) * sigma\n",
    "    dy = np.random.randn(*shape) * sigma\n",
    "    dz = np.random.randn(*shape) * sigma\n",
    "    \n",
    "    dx = gaussian_filter(dx, sigma, mode='constant') * alpha\n",
    "    dy = gaussian_filter(dy, sigma, mode='constant') * alpha\n",
    "    dz = gaussian_filter(dz, sigma, mode='constant') * alpha\n",
    "    \n",
    "    z, y, x = np.meshgrid(\n",
    "        np.arange(shape[0]),\n",
    "        np.arange(shape[1]),\n",
    "        np.arange(shape[2]),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    \n",
    "    indices = [\n",
    "        np.clip(z + dz, 0, shape[0] - 1).astype(int),\n",
    "        np.clip(y + dy, 0, shape[1] - 1).astype(int),\n",
    "        np.clip(x + dx, 0, shape[2] - 1).astype(int)\n",
    "    ]\n",
    "    \n",
    "    result = np.zeros_like(volume)\n",
    "    for c in range(volume.shape[0]):\n",
    "        result[c] = volume[c][indices[0], indices[1], indices[2]]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def resize_to_original(volume, target_shape):\n",
    "    current_shape = volume.shape\n",
    "    result = volume.copy()\n",
    "    \n",
    "    for i in range(len(target_shape)):\n",
    "        if current_shape[i] > target_shape[i]:\n",
    "            diff = current_shape[i] - target_shape[i]\n",
    "            start = diff // 2\n",
    "            end = start + target_shape[i]\n",
    "            result = np.take(result, range(start, end), axis=i)\n",
    "        elif current_shape[i] < target_shape[i]:\n",
    "            diff = target_shape[i] - current_shape[i]\n",
    "            pad_before = diff // 2\n",
    "            pad_after = diff - pad_before\n",
    "            pad_width = [(0, 0)] * len(target_shape)\n",
    "            pad_width[i] = (pad_before, pad_after)\n",
    "            result = np.pad(result, pad_width, mode='edge')\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Augmenting Class 1 (Aneurysm) samples...\")\n",
    "\n",
    "augmented_trainx = []\n",
    "augmented_trainy = []\n",
    "\n",
    "trainy_flat = [label.flatten()[0] if hasattr(label, 'flatten') else label for label in trainy]\n",
    "class1_indices = [i for i, label in enumerate(trainy_flat) if label == 1]\n",
    "print(f\"Found {len(class1_indices)} Class 1 samples\")\n",
    "\n",
    "num_augmentations_per_sample = 6\n",
    "\n",
    "print(\"Generating augmentations...\")\n",
    "for i, idx in enumerate(class1_indices):\n",
    "    if i % 30 == 0:\n",
    "        print(f\"  Processing sample {i+1}/{len(class1_indices)}\")\n",
    "    \n",
    "    original_volume = trainx[idx]\n",
    "    augmented_volumes = augment_3d_volume(original_volume, num_augmentations_per_sample)\n",
    "    \n",
    "    for aug_vol in augmented_volumes:\n",
    "        augmented_trainx.append(aug_vol)\n",
    "        augmented_trainy.append(1)\n",
    "\n",
    "print(f\"Generated {len(augmented_trainx)} augmented samples for Class 1\")\n",
    "\n",
    "trainx_combined = trainx + augmented_trainx\n",
    "\n",
    "trainy_combined_flat = []\n",
    "for label in trainy:\n",
    "    if isinstance(label, np.ndarray):\n",
    "        trainy_combined_flat.append(label.flatten()[0])\n",
    "    else:\n",
    "        trainy_combined_flat.append(label)\n",
    "        \n",
    "trainy_combined_flat.extend(augmented_trainy)\n",
    "\n",
    "trainx_tensor = tf.convert_to_tensor(trainx_combined, dtype=tf.float32)\n",
    "trainy_tensor = tf.convert_to_tensor(trainy_combined_flat, dtype=tf.float32)\n",
    "testx_tensor = tf.convert_to_tensor(testx, dtype=tf.float32)\n",
    "testy_tensor = tf.convert_to_tensor(testy, dtype=tf.float32)\n",
    "valx_tensor = tf.convert_to_tensor(valx, dtype=tf.float32)\n",
    "valy_tensor = tf.convert_to_tensor(valy, dtype=tf.float32)\n",
    "\n",
    "print(f\"\\nAugmented Training set shapes:\")\n",
    "print(f\"  X: {trainx_tensor.shape}\")\n",
    "print(f\"  y: {trainy_tensor.shape}\")\n",
    "\n",
    "train_labels_aug = np.array(trainy_combined_flat).flatten()\n",
    "unique_aug, counts_aug = np.unique(train_labels_aug, return_counts=True)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Class Distribution Comparison:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Original Training - Class 0: {counts_train[0]}, Class 1: {counts_train[1]}\")\n",
    "print(f\"                    Ratio: {counts_train[1]/counts_train[0]:.3f}\")\n",
    "print(f\"\\nAugmented Training - Class 0: {counts_aug[0]}, Class 1: {counts_aug[1]}\")\n",
    "print(f\"                     Ratio: {counts_aug[1]/counts_aug[0]:.3f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c5ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Data preprocessing\n",
    "trainx_tensor = tf.transpose(trainx_tensor, [0, 2, 3, 4, 1])\n",
    "valx_tensor = tf.transpose(valx_tensor, [0, 2, 3, 4, 1])\n",
    "testx_tensor = tf.transpose(testx_tensor, [0, 2, 3, 4, 1])\n",
    "\n",
    "print(f\"After transpose - Training data shape: {trainx_tensor.shape}\")\n",
    "\n",
    "trainx_norm = trainx_tensor / 255.0\n",
    "valx_norm = valx_tensor / 255.0\n",
    "testx_norm = testx_tensor / 255.0\n",
    "\n",
    "trainy_flat = tf.squeeze(trainy_tensor)\n",
    "valy_flat = tf.squeeze(valy_tensor)\n",
    "testy_flat = tf.squeeze(testy_tensor)\n",
    "\n",
    "print(f\"Normalized training data shape: {trainx_norm.shape}\")\n",
    "print(f\"Training labels shape: {trainy_flat.shape}\")\n",
    "print(f\"Data range: [{tf.reduce_min(trainx_norm):.3f}, {tf.reduce_max(trainx_norm):.3f}]\")\n",
    "\n",
    "train_labels_current = np.array(trainy_flat).flatten()\n",
    "unique_current, counts_current = np.unique(train_labels_current, return_counts=True)\n",
    "class_weight = {\n",
    "    0: len(train_labels_current) / (2 * counts_current[0]),\n",
    "    1: len(train_labels_current) / (2 * counts_current[1])\n",
    "}\n",
    "print(f\"\\nClass weights to handle imbalance: {class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Build model\n",
    "def build_3d_cnn(input_shape=(28, 28, 28, 1), dropout_rate=0.3):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        layers.Conv3D(32, kernel_size=(3, 3, 3), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Conv3D(64, kernel_size=(3, 3, 3), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Conv3D(128, kernel_size=(3, 3, 3), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.GlobalAveragePooling3D(),\n",
    "        \n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_3d_cnn(input_shape=(28, 28, 28, 1), dropout_rate=0.3)\n",
    "model.summary()\n",
    "\n",
    "print(\"\\nTotal parameters:\", model.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4723280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"Model compiled with:\")\n",
    "print(\"- Optimizer: Adam (lr=0.001)\")\n",
    "print(\"- Loss: Binary Crossentropy\")\n",
    "print(\"- Metrics: Accuracy, AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e21b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Setup callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'best_vessel_model.h5',\n",
    "    monitor='val_auc',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"- Early Stopping: patience=15, monitor=val_auc\")\n",
    "print(\"- ReduceLROnPlateau: factor=0.5, patience=5\")\n",
    "print(\"- ModelCheckpoint: saves best val_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a414ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Quick test (2 epochs)\n",
    "history = model.fit(\n",
    "    trainx_norm, trainy_flat,\n",
    "    batch_size=16,\n",
    "    epochs=2,\n",
    "    validation_data=(valx_norm, valy_flat),\n",
    "    class_weight=class_weight,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INITIAL TEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Epoch 1 - Training Loss: {history.history['loss'][0]:.4f}, Val Loss: {history.history['val_loss'][0]:.4f}\")\n",
    "print(f\"Epoch 2 - Training Loss: {history.history['loss'][1]:.4f}, Val Loss: {history.history['val_loss'][1]:.4f}\")\n",
    "print(f\"\\nTraining AUC: {history.history['auc'][-1]:.4f}\")\n",
    "print(f\"Validation AUC: {history.history['val_auc'][-1]:.4f}\")\n",
    "\n",
    "if history.history['loss'][1] < history.history['loss'][0]:\n",
    "    print(\"\\nLoss is decreasing - model is learning!\")\n",
    "else:\n",
    "    print(\"\\nLoss not decreasing - may need to adjust hyperparameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a8c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Full training\n",
    "print(\"Starting full training session...\")\n",
    "\n",
    "history_full = model.fit(\n",
    "    trainx_norm, trainy_flat,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(valx_norm, valy_flat),\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(history_full.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history_full.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Model Loss Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_full.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history_full.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Model Accuracy Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "axes[2].plot(history_full.history['auc'], label='Training AUC', linewidth=2)\n",
    "axes[2].plot(history_full.history['val_auc'], label='Validation AUC', linewidth=2)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('AUC')\n",
    "axes[2].set_title('Model AUC Over Time')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b120b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = keras.models.load_model('best_vessel_model.h5')\n",
    "\n",
    "test_loss, test_acc, test_auc = model.evaluate(testx_norm, testy_flat, verbose=0)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "y_pred_proba = model.predict(testx_norm, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, y_pred, \n",
    "                          target_names=['Healthy', 'Aneurysm'],\n",
    "                          digits=4))\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Healthy', 'Aneurysm'],\n",
    "            yticklabels=['Healthy', 'Aneurysm'])\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'Model (AUC = {test_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Compare to baseline\n",
    "print(\"VesselMNIST3D Baseline Performance (from MedMNIST paper):\")\n",
    "print(\"- ResNet18 (3D): AUC ~0.920, ACC ~0.890\")\n",
    "print(\"- Auto-sklearn: AUC ~0.917, ACC ~0.887\")\n",
    "print(\"\\nYour Model Performance:\")\n",
    "print(f\"- Test AUC: {test_auc:.4f}\")\n",
    "print(f\"- Test ACC: {test_acc:.4f}\")\n",
    "\n",
    "if test_auc >= 0.920:\n",
    "    print(\"\\nModel matches or exceeds the baseline!\")\n",
    "elif test_auc >= 0.900:\n",
    "    print(\"\\nGood performance! Close to the baseline.\")\n",
    "else:\n",
    "    print(\"\\nRoom for improvement\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
