{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e6f156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 GPU(s)\n",
      "  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "  GPU 1: PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n",
      "\n",
      "Attempting to use all available GPUs with memory growth enabled\n",
      "\n",
      "TensorFlow version: 2.20.0\n",
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and setup (MODIFIED - Flexible GPU selection)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.widgets import Slider\n",
    "import matplotlib\n",
    "import matplotlib.font_manager\n",
    "from medmnist import VesselMNIST3D\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        print(f\"Found {len(gpus)} GPU(s)\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"  GPU {i}: {gpu}\")\n",
    "        \n",
    "        print(\"\\nAttempting to use all available GPUs with memory growth enabled\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"No GPUs found, using CPU\")\n",
    "\n",
    "print(\"\\nTensorFlow version:\", tf.__version__)\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117fa961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Training samples: 1335\n",
      "Validation samples: 191\n",
      "Test samples: 382\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load data\n",
    "train_dataset = VesselMNIST3D(split='train', size=28, download=True)\n",
    "trainx = []\n",
    "trainy = []\n",
    "\n",
    "test_dataset = VesselMNIST3D(split='test', size=28, download=True)\n",
    "testx = []\n",
    "testy = []\n",
    "\n",
    "val_dataset = VesselMNIST3D(split='val', size=28, download=True)\n",
    "valx = []\n",
    "valy = []\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    trainx.append(train_dataset[i][0])\n",
    "    trainy.append(train_dataset[i][1])\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    testx.append(test_dataset[i][0])\n",
    "    testy.append(test_dataset[i][1])\n",
    "\n",
    "for i in range(len(val_dataset)):\n",
    "    valx.append(val_dataset[i][0])\n",
    "    valy.append(val_dataset[i][1])\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Training samples: {len(trainx)}\")\n",
    "print(f\"Validation samples: {len(valx)}\")\n",
    "print(f\"Test samples: {len(testx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f67789bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Class 0 (Healthy): 1185, Class 1 (Aneurysm): 150\n",
      "Validation - Class 0 (Healthy): 169, Class 1 (Aneurysm): 22\n",
      "Test - Class 0 (Healthy): 339, Class 1 (Aneurysm): 43\n",
      "\n",
      "Class imbalance ratio (train): 0.127\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Analyze original class distribution\n",
    "train_labels = np.array(trainy).flatten()\n",
    "val_labels = np.array(valy).flatten()\n",
    "test_labels = np.array(testy).flatten()\n",
    "\n",
    "unique_train, counts_train = np.unique(train_labels, return_counts=True)\n",
    "unique_val, counts_val = np.unique(val_labels, return_counts=True)\n",
    "unique_test, counts_test = np.unique(test_labels, return_counts=True)\n",
    "\n",
    "print(f\"Training - Class 0 (Healthy): {counts_train[0]}, Class 1 (Aneurysm): {counts_train[1]}\")\n",
    "print(f\"Validation - Class 0 (Healthy): {counts_val[0]}, Class 1 (Aneurysm): {counts_val[1]}\")\n",
    "print(f\"Test - Class 0 (Healthy): {counts_test[0]}, Class 1 (Aneurysm): {counts_test[1]}\")\n",
    "print(f\"\\nClass imbalance ratio (train): {counts_train[1]/counts_train[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36153f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting Class 1 (Aneurysm) samples...\n",
      "Found 150 Class 1 samples\n",
      "Generating augmentations...\n",
      "  Processing sample 1/150\n",
      "  Processing sample 31/150\n",
      "  Processing sample 61/150\n",
      "  Processing sample 91/150\n",
      "  Processing sample 121/150\n",
      "Generated 900 augmented samples for Class 1\n",
      "\n",
      "Augmented Training set shapes:\n",
      "  X: (2235, 1, 28, 28, 28)\n",
      "  y: (2235,)\n",
      "\n",
      "============================================================\n",
      "Class Distribution Comparison:\n",
      "============================================================\n",
      "Original Training - Class 0: 1185, Class 1: 150\n",
      "                    Ratio: 0.127\n",
      "\n",
      "Augmented Training - Class 0: 1185, Class 1: 1050\n",
      "                     Ratio: 0.886\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765558369.457243   66763 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 958 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:41:00.0, compute capability: 8.9\n",
      "I0000 00:00:1765558369.458571   66763 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 21759 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:61:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Augmentation functions and execution\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate, zoom, shift\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import tensorflow as tf\n",
    "\n",
    "def augment_3d_volume(volume, num_augmentations=5):\n",
    "    augmented_volumes = []\n",
    "    \n",
    "    if hasattr(volume, 'numpy'):\n",
    "        volume = volume.numpy()\n",
    "    \n",
    "    original_shape = volume.shape\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "        aug_volume = volume.copy()\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            angle = np.random.uniform(-15, 15)\n",
    "            axes_options = [(1, 2), (1, 3), (2, 3)]\n",
    "            axes = axes_options[np.random.randint(0, len(axes_options))]\n",
    "            aug_volume = rotate(aug_volume, angle, axes=axes, reshape=False, mode='nearest')\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            scale_factor = np.random.uniform(0.9, 1.1)\n",
    "            zoom_factors = [1, scale_factor, scale_factor, scale_factor]\n",
    "            aug_volume = zoom(aug_volume, zoom_factors, mode='nearest')\n",
    "            aug_volume = resize_to_original(aug_volume, original_shape)\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            axis = np.random.randint(1, 4)\n",
    "            aug_volume = np.flip(aug_volume, axis=axis)\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            shift_amount = [0] + [np.random.randint(-3, 4) for _ in range(3)]\n",
    "            aug_volume = shift(aug_volume, shift_amount, mode='nearest')\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            noise = np.random.normal(0, 0.01, aug_volume.shape)\n",
    "            aug_volume = aug_volume + noise\n",
    "            aug_volume = np.clip(aug_volume, 0, 1)\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            brightness_factor = np.random.uniform(0.9, 1.1)\n",
    "            aug_volume = aug_volume * brightness_factor\n",
    "            aug_volume = np.clip(aug_volume, 0, 1)\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            mean = np.mean(aug_volume)\n",
    "            aug_volume = (aug_volume - mean) * np.random.uniform(0.9, 1.1) + mean\n",
    "            aug_volume = np.clip(aug_volume, 0, 1)\n",
    "        \n",
    "        if np.random.rand() > 0.7:\n",
    "            aug_volume = elastic_transform_3d(aug_volume, alpha=5, sigma=3)\n",
    "        \n",
    "        augmented_volumes.append(aug_volume)\n",
    "    \n",
    "    return augmented_volumes\n",
    "\n",
    "def elastic_transform_3d(volume, alpha=10, sigma=3):\n",
    "    shape = volume.shape[1:]\n",
    "    \n",
    "    dx = np.random.randn(*shape) * sigma\n",
    "    dy = np.random.randn(*shape) * sigma\n",
    "    dz = np.random.randn(*shape) * sigma\n",
    "    \n",
    "    dx = gaussian_filter(dx, sigma, mode='constant') * alpha\n",
    "    dy = gaussian_filter(dy, sigma, mode='constant') * alpha\n",
    "    dz = gaussian_filter(dz, sigma, mode='constant') * alpha\n",
    "    \n",
    "    z, y, x = np.meshgrid(\n",
    "        np.arange(shape[0]),\n",
    "        np.arange(shape[1]),\n",
    "        np.arange(shape[2]),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    \n",
    "    indices = [\n",
    "        np.clip(z + dz, 0, shape[0] - 1).astype(int),\n",
    "        np.clip(y + dy, 0, shape[1] - 1).astype(int),\n",
    "        np.clip(x + dx, 0, shape[2] - 1).astype(int)\n",
    "    ]\n",
    "    \n",
    "    result = np.zeros_like(volume)\n",
    "    for c in range(volume.shape[0]):\n",
    "        result[c] = volume[c][indices[0], indices[1], indices[2]]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def resize_to_original(volume, target_shape):\n",
    "    current_shape = volume.shape\n",
    "    result = volume.copy()\n",
    "    \n",
    "    for i in range(len(target_shape)):\n",
    "        if current_shape[i] > target_shape[i]:\n",
    "            diff = current_shape[i] - target_shape[i]\n",
    "            start = diff // 2\n",
    "            end = start + target_shape[i]\n",
    "            result = np.take(result, range(start, end), axis=i)\n",
    "        elif current_shape[i] < target_shape[i]:\n",
    "            diff = target_shape[i] - current_shape[i]\n",
    "            pad_before = diff // 2\n",
    "            pad_after = diff - pad_before\n",
    "            pad_width = [(0, 0)] * len(target_shape)\n",
    "            pad_width[i] = (pad_before, pad_after)\n",
    "            result = np.pad(result, pad_width, mode='edge')\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Augmenting Class 1 (Aneurysm) samples...\")\n",
    "\n",
    "augmented_trainx = []\n",
    "augmented_trainy = []\n",
    "\n",
    "trainy_flat = [label.flatten()[0] if hasattr(label, 'flatten') else label for label in trainy]\n",
    "class1_indices = [i for i, label in enumerate(trainy_flat) if label == 1]\n",
    "print(f\"Found {len(class1_indices)} Class 1 samples\")\n",
    "\n",
    "num_augmentations_per_sample = 6\n",
    "\n",
    "print(\"Generating augmentations...\")\n",
    "for i, idx in enumerate(class1_indices):\n",
    "    if i % 30 == 0:\n",
    "        print(f\"  Processing sample {i+1}/{len(class1_indices)}\")\n",
    "    \n",
    "    original_volume = trainx[idx]\n",
    "    augmented_volumes = augment_3d_volume(original_volume, num_augmentations_per_sample)\n",
    "    \n",
    "    for aug_vol in augmented_volumes:\n",
    "        augmented_trainx.append(aug_vol)\n",
    "        augmented_trainy.append(1)\n",
    "\n",
    "print(f\"Generated {len(augmented_trainx)} augmented samples for Class 1\")\n",
    "\n",
    "trainx_combined = trainx + augmented_trainx\n",
    "\n",
    "trainy_combined_flat = []\n",
    "for label in trainy:\n",
    "    if isinstance(label, np.ndarray):\n",
    "        trainy_combined_flat.append(label.flatten()[0])\n",
    "    else:\n",
    "        trainy_combined_flat.append(label)\n",
    "        \n",
    "trainy_combined_flat.extend(augmented_trainy)\n",
    "\n",
    "trainx_tensor = tf.convert_to_tensor(trainx_combined, dtype=tf.float32)\n",
    "trainy_tensor = tf.convert_to_tensor(trainy_combined_flat, dtype=tf.float32)\n",
    "testx_tensor = tf.convert_to_tensor(testx, dtype=tf.float32)\n",
    "testy_tensor = tf.convert_to_tensor(testy, dtype=tf.float32)\n",
    "valx_tensor = tf.convert_to_tensor(valx, dtype=tf.float32)\n",
    "valy_tensor = tf.convert_to_tensor(valy, dtype=tf.float32)\n",
    "\n",
    "print(f\"\\nAugmented Training set shapes:\")\n",
    "print(f\"  X: {trainx_tensor.shape}\")\n",
    "print(f\"  y: {trainy_tensor.shape}\")\n",
    "\n",
    "train_labels_aug = np.array(trainy_combined_flat).flatten()\n",
    "unique_aug, counts_aug = np.unique(train_labels_aug, return_counts=True)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Class Distribution Comparison:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Original Training - Class 0: {counts_train[0]}, Class 1: {counts_train[1]}\")\n",
    "print(f\"                    Ratio: {counts_train[1]/counts_train[0]:.3f}\")\n",
    "print(f\"\\nAugmented Training - Class 0: {counts_aug[0]}, Class 1: {counts_aug[1]}\")\n",
    "print(f\"                     Ratio: {counts_aug[1]/counts_aug[0]:.3f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de153ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02c5ef3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After transpose - Training data shape: (2235, 28, 28, 28, 1)\n",
      "Normalized training data shape: (2235, 28, 28, 28, 1)\n",
      "Training labels shape: (2235,)\n",
      "Data range: [-0.002, 0.006]\n",
      "\n",
      "Class weights to handle imbalance: {0: np.float64(0.9430379746835443), 1: np.float64(1.0642857142857143)}\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Data preprocessing\n",
    "trainx_tensor = tf.transpose(trainx_tensor, [0, 2, 3, 4, 1])\n",
    "valx_tensor = tf.transpose(valx_tensor, [0, 2, 3, 4, 1])\n",
    "testx_tensor = tf.transpose(testx_tensor, [0, 2, 3, 4, 1])\n",
    "\n",
    "print(f\"After transpose - Training data shape: {trainx_tensor.shape}\")\n",
    "\n",
    "trainx_norm = trainx_tensor / 255.0\n",
    "valx_norm = valx_tensor / 255.0\n",
    "testx_norm = testx_tensor / 255.0\n",
    "\n",
    "trainy_flat = tf.squeeze(trainy_tensor)\n",
    "valy_flat = tf.squeeze(valy_tensor)\n",
    "testy_flat = tf.squeeze(testy_tensor)\n",
    "\n",
    "print(f\"Normalized training data shape: {trainx_norm.shape}\")\n",
    "print(f\"Training labels shape: {trainy_flat.shape}\")\n",
    "print(f\"Data range: [{tf.reduce_min(trainx_norm):.3f}, {tf.reduce_max(trainx_norm):.3f}]\")\n",
    "\n",
    "train_labels_current = np.array(trainy_flat).flatten()\n",
    "unique_current, counts_current = np.unique(train_labels_current, return_counts=True)\n",
    "class_weight = {\n",
    "    0: len(train_labels_current) / (2 * counts_current[0]),\n",
    "    1: len(train_labels_current) / (2 * counts_current[1])\n",
    "}\n",
    "print(f\"\\nClass weights to handle imbalance: {class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97a0ac44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">55,360</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">221,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling3d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling3D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv3d (\u001b[38;5;33mConv3D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m) │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m) │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d (\u001b[38;5;33mMaxPooling3D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_1 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m) │        \u001b[38;5;34m55,360\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m) │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_1 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_2 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │       \u001b[38;5;34m221,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_2 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling3d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling3D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">328,001</span> (1.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m328,001\u001b[0m (1.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">327,553</span> (1.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m327,553\u001b[0m (1.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total parameters: 328001\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Build model\n",
    "def build_3d_cnn(input_shape=(28, 28, 28, 1), dropout_rate=0.3):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        layers.Conv3D(32, kernel_size=(3, 3, 3), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Conv3D(64, kernel_size=(3, 3, 3), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Conv3D(128, kernel_size=(3, 3, 3), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.GlobalAveragePooling3D(),\n",
    "        \n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_3d_cnn(input_shape=(28, 28, 28, 1), dropout_rate=0.3)\n",
    "model.summary()\n",
    "\n",
    "print(\"\\nTotal parameters:\", model.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4723280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled with:\n",
      "- Optimizer: Adam (lr=0.001)\n",
      "- Loss: Binary Crossentropy\n",
      "- Metrics: Accuracy, AUC\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"Model compiled with:\")\n",
    "print(\"- Optimizer: Adam (lr=0.001)\")\n",
    "print(\"- Loss: Binary Crossentropy\")\n",
    "print(\"- Metrics: Accuracy, AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50e21b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks configured:\n",
      "- Early Stopping: patience=15, monitor=val_auc\n",
      "- ReduceLROnPlateau: factor=0.5, patience=5\n",
      "- ModelCheckpoint: saves best val_auc\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Setup callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'best_vessel_model.h5',\n",
    "    monitor='val_auc',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"- Early Stopping: patience=15, monitor=val_auc\")\n",
    "print(\"- ReduceLROnPlateau: factor=0.5, patience=5\")\n",
    "print(\"- ModelCheckpoint: saves best val_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62a414ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 11:54:52.277929: I external/local_xla/xla/service/service.cc:163] XLA service 0x711bf4002460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-12 11:54:52.277943: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2025-12-12 11:54:52.277947: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (1): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2025-12-12 11:54:52.331717: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-12 11:54:52.657580: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n",
      "2025-12-12 11:54:53.427224: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 340.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-12-12 11:54:53.450837: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 340.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 65/280\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5749 - auc: 0.5994 - loss: 0.7146"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765558496.836916   78892 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m265/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6362 - auc: 0.6908 - loss: 0.6410"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 11:54:58.455749: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 317.50MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6397 - auc: 0.6957 - loss: 0.6366"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 11:55:02.202258: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_170', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-12-12 11:55:02.255379: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 340.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-12-12 11:55:03.100307: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_170', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-12-12 11:55:03.141263: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 335.50MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 24ms/step - accuracy: 0.7007 - auc: 0.7801 - loss: 0.5631 - val_accuracy: 0.1152 - val_auc: 0.7944 - val_loss: 2.8020\n",
      "Epoch 2/2\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8063 - auc: 0.8959 - loss: 0.3966 - val_accuracy: 0.1152 - val_auc: 0.5237 - val_loss: 7.3217\n",
      "\n",
      "======================================================================\n",
      "INITIAL TEST RESULTS\n",
      "======================================================================\n",
      "Epoch 1 - Training Loss: 0.5631, Val Loss: 2.8020\n",
      "Epoch 2 - Training Loss: 0.3966, Val Loss: 7.3217\n",
      "\n",
      "Training AUC: 0.8959\n",
      "Validation AUC: 0.5237\n",
      "\n",
      "Loss is decreasing - model is learning!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Quick test (2 epochs)\n",
    "history = model.fit(\n",
    "    trainx_norm, trainy_flat,\n",
    "    batch_size=8,\n",
    "    epochs=2,\n",
    "    validation_data=(valx_norm, valy_flat),\n",
    "    class_weight=class_weight,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INITIAL TEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Epoch 1 - Training Loss: {history.history['loss'][0]:.4f}, Val Loss: {history.history['val_loss'][0]:.4f}\")\n",
    "print(f\"Epoch 2 - Training Loss: {history.history['loss'][1]:.4f}, Val Loss: {history.history['val_loss'][1]:.4f}\")\n",
    "print(f\"\\nTraining AUC: {history.history['auc'][-1]:.4f}\")\n",
    "print(f\"Validation AUC: {history.history['val_auc'][-1]:.4f}\")\n",
    "\n",
    "if history.history['loss'][1] < history.history['loss'][0]:\n",
    "    print(\"\\nLoss is decreasing - model is learning!\")\n",
    "else:\n",
    "    print(\"\\nLoss not decreasing - may need to adjust hyperparameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2a8c4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full training session...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 11:55:16.115321: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 11:55:16.115356: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 11:55:16.115368: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 11:55:16.115377: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 11:55:16.115386: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 11:55:16.115403: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 11:55:16.705899: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3222', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-12-12 11:55:16.844544: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3222', 496 bytes spill stores, 496 bytes spill loads\n",
      "\n",
      "2025-12-12 11:55:17.094307: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2274', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-12-12 11:55:17.217661: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3563', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-12-12 11:55:17.405889: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3563', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-12-12 11:55:17.561130: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3579', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-12-12 11:55:17.632711: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 104.43MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-12-12 11:55:17.632744: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 104.43MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-12-12 11:55:17.690648: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 448.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-12-12 11:55:17.704919: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 448.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-12-12 11:55:30.945883: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 347.75MiB (rounded to 364637440)requested by op \n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-12-12 11:55:30.945918: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1049] BFCAllocator dump for GPU_0_bfc\n",
      "2025-12-12 11:55:30.945932: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (256): \tTotal Chunks: 64, Chunks in use: 64. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 6.1KiB client-requested in use in bin.\n",
      "2025-12-12 11:55:30.945940: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (512): \tTotal Chunks: 13, Chunks in use: 13. 7.0KiB allocated for chunks. 7.0KiB in use in bin. 6.8KiB client-requested in use in bin.\n",
      "2025-12-12 11:55:30.945948: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (1024): \tTotal Chunks: 9, Chunks in use: 9. 11.5KiB allocated for chunks. 11.5KiB in use in bin. 8.6KiB client-requested in use in bin.\n",
      "2025-12-12 11:55:30.945956: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (2048): \tTotal Chunks: 3, Chunks in use: 3. 10.5KiB allocated for chunks. 10.5KiB in use in bin. 10.1KiB client-requested in use in bin.\n",
      "2025-12-12 11:55:30.945964: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-12 11:55:30.945973: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (8192): \tTotal Chunks: 2, Chunks in use: 2. 17.5KiB allocated for chunks. 17.5KiB in use in bin. 17.5KiB client-requested in use in bin.\n",
      "2025-12-12 11:55:30.945979: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-12 11:55:30.945986: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-12 11:55:30.945994: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (65536): \tTotal Chunks: 4, Chunks in use: 3. 364.0KiB allocated for chunks. 254.5KiB in use in bin. 192.0KiB client-requested in use in bin.\n",
      "2025-12-12 11:55:30.946002: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (131072): \tTotal Chunks: 5, Chunks in use: 5. 864.0KiB allocated for chunks. 864.0KiB in use in bin. 816.0KiB client-requested in use in bin.\n",
      "2025-12-12 11:55:30.946010: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (262144): \tTotal Chunks: 1, Chunks in use: 1. 291.8KiB allocated for chunks. 291.8KiB in use in bin. 216.0KiB client-requested in use in bin.\n",
      "2025-12-12 11:55:30.946018: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (524288): \tTotal Chunks: 3, Chunks in use: 3. 2.53MiB allocated for chunks. 2.53MiB in use in bin. 2.53MiB client-requested in use in bin.\n",
      "2025-12-12 11:55:30.946025: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (1048576): \tTotal Chunks: 1, Chunks in use: 0. 1.18MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-12 11:55:30.946032: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 1. 2.68MiB allocated for chunks. 2.68MiB in use in bin. 2.68MiB client-requested in use in bin.\n",
      "2025-12-12 11:55:30.946039: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-12 11:55:30.946047: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (8388608): \tTotal Chunks: 2, Chunks in use: 2. 31.99MiB allocated for chunks. 31.99MiB in use in bin. 31.99MiB client-requested in use in bin.\n",
      "2025-12-12 11:55:30.946055: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (16777216): \tTotal Chunks: 2, Chunks in use: 2. 63.98MiB allocated for chunks. 63.98MiB in use in bin. 63.98MiB client-requested in use in bin.\n",
      "2025-12-12 11:55:30.946061: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-12 11:55:30.946074: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 0. 66.56MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-12 11:55:30.946082: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 208.00MiB allocated for chunks. 208.00MiB in use in bin. 187.16MiB client-requested in use in bin.\n",
      "2025-12-12 11:55:30.946090: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (268435456): \tTotal Chunks: 2, Chunks in use: 1. 579.66MiB allocated for chunks. 256.00MiB in use in bin. 187.16MiB client-requested in use in bin.\n",
      "2025-12-12 11:55:30.946098: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1072] Bin for 347.75MiB was 256.00MiB, Chunk State: \n",
      "2025-12-12 11:55:30.946109: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1078]   Size: 323.66MiB | Requested Size: 16.00MiB | in_use: 0 | bin_num: 20, prev:   Size: 256B | Requested Size: 4B | in_use: 1 | bin_num: -1\n",
      "2025-12-12 11:55:30.946115: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1085] Next region of size 467795968\n",
      "2025-12-12 11:55:30.946123: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8c000000 of size 16771328 next 10\n",
      "2025-12-12 11:55:30.946129: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8cffe900 of size 33542656 next 11\n",
      "2025-12-12 11:55:30.946135: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effbb00 of size 256 next 12\n",
      "2025-12-12 11:55:30.946143: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effbc00 of size 256 next 13\n",
      "2025-12-12 11:55:30.946149: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effbd00 of size 256 next 14\n",
      "2025-12-12 11:55:30.946154: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effbe00 of size 256 next 15\n",
      "2025-12-12 11:55:30.946159: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effbf00 of size 256 next 16\n",
      "2025-12-12 11:55:30.946165: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc000 of size 256 next 17\n",
      "2025-12-12 11:55:30.946182: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc100 of size 256 next 18\n",
      "2025-12-12 11:55:30.946187: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc200 of size 256 next 19\n",
      "2025-12-12 11:55:30.946193: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc300 of size 256 next 20\n",
      "2025-12-12 11:55:30.946198: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc400 of size 256 next 22\n",
      "2025-12-12 11:55:30.946203: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc500 of size 256 next 23\n",
      "2025-12-12 11:55:30.946209: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc600 of size 256 next 21\n",
      "2025-12-12 11:55:30.946214: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc700 of size 256 next 24\n",
      "2025-12-12 11:55:30.946219: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc800 of size 256 next 27\n",
      "2025-12-12 11:55:30.946225: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc900 of size 256 next 28\n",
      "2025-12-12 11:55:30.946230: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effca00 of size 256 next 29\n",
      "2025-12-12 11:55:30.946235: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effcb00 of size 256 next 30\n",
      "2025-12-12 11:55:30.946241: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effcc00 of size 256 next 31\n",
      "2025-12-12 11:55:30.946246: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effcd00 of size 256 next 34\n",
      "2025-12-12 11:55:30.946253: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effce00 of size 256 next 32\n",
      "2025-12-12 11:55:30.946259: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effcf00 of size 256 next 33\n",
      "2025-12-12 11:55:30.946264: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effd000 of size 256 next 37\n",
      "2025-12-12 11:55:30.946269: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effd100 of size 256 next 38\n",
      "2025-12-12 11:55:30.946275: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effd200 of size 256 next 39\n",
      "2025-12-12 11:55:30.946280: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effd300 of size 512 next 40\n",
      "2025-12-12 11:55:30.946286: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effd500 of size 512 next 43\n",
      "2025-12-12 11:55:30.946292: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effd700 of size 512 next 41\n",
      "2025-12-12 11:55:30.946297: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effd900 of size 512 next 42\n",
      "2025-12-12 11:55:30.946303: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effdb00 of size 512 next 46\n",
      "2025-12-12 11:55:30.946307: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effdd00 of size 256 next 47\n",
      "2025-12-12 11:55:30.946311: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effde00 of size 256 next 48\n",
      "2025-12-12 11:55:30.946314: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effdf00 of size 1024 next 25\n",
      "2025-12-12 11:55:30.946316: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effe300 of size 3584 next 26\n",
      "2025-12-12 11:55:30.946318: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff100 of size 256 next 49\n",
      "2025-12-12 11:55:30.946320: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff200 of size 256 next 52\n",
      "2025-12-12 11:55:30.946323: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff300 of size 256 next 50\n",
      "2025-12-12 11:55:30.946325: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff400 of size 256 next 54\n",
      "2025-12-12 11:55:30.946327: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff500 of size 256 next 55\n",
      "2025-12-12 11:55:30.946329: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff600 of size 256 next 51\n",
      "2025-12-12 11:55:30.946331: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff700 of size 256 next 58\n",
      "2025-12-12 11:55:30.946333: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff800 of size 256 next 56\n",
      "2025-12-12 11:55:30.946335: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff900 of size 256 next 59\n",
      "2025-12-12 11:55:30.946337: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efffa00 of size 256 next 60\n",
      "2025-12-12 11:55:30.946340: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efffb00 of size 256 next 84\n",
      "2025-12-12 11:55:30.946342: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efffc00 of size 256 next 74\n",
      "2025-12-12 11:55:30.946344: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efffd00 of size 256 next 126\n",
      "2025-12-12 11:55:30.946346: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efffe00 of size 256 next 61\n",
      "2025-12-12 11:55:30.946348: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effff00 of size 1792 next 87\n",
      "2025-12-12 11:55:30.946350: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000600 of size 1792 next 66\n",
      "2025-12-12 11:55:30.946353: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000d00 of size 256 next 67\n",
      "2025-12-12 11:55:30.946355: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000e00 of size 256 next 68\n",
      "2025-12-12 11:55:30.946357: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000f00 of size 8960 next 69\n",
      "2025-12-12 11:55:30.946359: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f003200 of size 256 next 70\n",
      "2025-12-12 11:55:30.946361: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f003300 of size 256 next 81\n",
      "2025-12-12 11:55:30.946363: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f003400 of size 1280 next 76\n",
      "2025-12-12 11:55:30.946366: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f003900 of size 1024 next 125\n",
      "2025-12-12 11:55:30.946368: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f003d00 of size 768 next 64\n",
      "2025-12-12 11:55:30.946370: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f004000 of size 256 next 73\n",
      "2025-12-12 11:55:30.946372: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 711b8f004100 of size 112128 next 57\n",
      "2025-12-12 11:55:30.946374: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f01f700 of size 129536 next 53\n",
      "2025-12-12 11:55:30.946378: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f03f100 of size 180224 next 36\n",
      "2025-12-12 11:55:30.946380: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f06b100 of size 221184 next 35\n",
      "2025-12-12 11:55:30.946383: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 711b8f0a1100 of size 1240832 next 89\n",
      "2025-12-12 11:55:30.946385: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f1d0000 of size 3584 next 90\n",
      "2025-12-12 11:55:30.946387: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f1d0e00 of size 3584 next 91\n",
      "2025-12-12 11:55:30.946389: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f1d1c00 of size 256 next 92\n",
      "2025-12-12 11:55:30.946391: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f1d1d00 of size 256 next 93\n",
      "2025-12-12 11:55:30.946393: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f1d1e00 of size 256 next 94\n",
      "2025-12-12 11:55:30.946395: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f1d1f00 of size 256 next 95\n",
      "2025-12-12 11:55:30.946398: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f1d2000 of size 256 next 96\n",
      "2025-12-12 11:55:30.946400: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f1d2100 of size 256 next 97\n",
      "2025-12-12 11:55:30.946402: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f1d2200 of size 221184 next 98\n",
      "2025-12-12 11:55:30.946404: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f208200 of size 298752 next 45\n",
      "2025-12-12 11:55:30.946406: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f251100 of size 884736 next 44\n",
      "2025-12-12 11:55:30.946409: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f329100 of size 2809856 next 71\n",
      "2025-12-12 11:55:30.946411: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 711b8f5d7100 of size 69792512 next 83\n",
      "2025-12-12 11:55:30.946414: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93866400 of size 256 next 99\n",
      "2025-12-12 11:55:30.946416: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93866500 of size 256 next 100\n",
      "2025-12-12 11:55:30.946418: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93866600 of size 256 next 101\n",
      "2025-12-12 11:55:30.946420: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93866700 of size 256 next 102\n",
      "2025-12-12 11:55:30.946422: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93866800 of size 256 next 103\n",
      "2025-12-12 11:55:30.946424: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93866900 of size 256 next 104\n",
      "2025-12-12 11:55:30.946426: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93866a00 of size 884736 next 105\n",
      "2025-12-12 11:55:30.946429: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b9393ea00 of size 884736 next 106\n",
      "2025-12-12 11:55:30.946431: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a16a00 of size 512 next 107\n",
      "2025-12-12 11:55:30.946433: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a16c00 of size 512 next 108\n",
      "2025-12-12 11:55:30.946435: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a16e00 of size 512 next 109\n",
      "2025-12-12 11:55:30.946437: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a17000 of size 512 next 110\n",
      "2025-12-12 11:55:30.946439: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a17200 of size 512 next 111\n",
      "2025-12-12 11:55:30.946441: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a17400 of size 512 next 112\n",
      "2025-12-12 11:55:30.946444: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a17600 of size 131072 next 113\n",
      "2025-12-12 11:55:30.946446: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a37600 of size 131072 next 114\n",
      "2025-12-12 11:55:30.946448: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a57600 of size 1024 next 115\n",
      "2025-12-12 11:55:30.946451: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a57a00 of size 1024 next 116\n",
      "2025-12-12 11:55:30.946453: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a57e00 of size 65536 next 117\n",
      "2025-12-12 11:55:30.946455: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a67e00 of size 65536 next 118\n",
      "2025-12-12 11:55:30.946457: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a77e00 of size 256 next 119\n",
      "2025-12-12 11:55:30.946459: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a77f00 of size 256 next 120\n",
      "2025-12-12 11:55:30.946461: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a78000 of size 256 next 121\n",
      "2025-12-12 11:55:30.946464: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a78100 of size 256 next 122\n",
      "2025-12-12 11:55:30.946466: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a78200 of size 256 next 123\n",
      "2025-12-12 11:55:30.946468: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b93a78300 of size 256 next 124\n",
      "2025-12-12 11:55:30.946470: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 711b93a78400 of size 339377152 next 18446744073709551615\n",
      "2025-12-12 11:55:30.946472: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1085] Next region of size 268435456\n",
      "2025-12-12 11:55:30.946475: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711ba8000000 of size 1280 next 2\n",
      "2025-12-12 11:55:30.946477: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711ba8000500 of size 8960 next 3\n",
      "2025-12-12 11:55:30.946479: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711ba8002800 of size 33542656 next 4\n",
      "2025-12-12 11:55:30.946481: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711ba9fffa00 of size 1536 next 5\n",
      "2025-12-12 11:55:30.946485: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa000000 of size 16771328 next 6\n",
      "2025-12-12 11:55:30.946487: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baaffe900 of size 768 next 7\n",
      "2025-12-12 11:55:30.946490: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baaffec00 of size 256 next 8\n",
      "2025-12-12 11:55:30.946492: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baaffed00 of size 218108672 next 18446744073709551615\n",
      "2025-12-12 11:55:30.946494: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1085] Next region of size 268435456\n",
      "2025-12-12 11:55:30.946496: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711bc8000000 of size 268435456 next 18446744073709551615\n",
      "2025-12-12 11:55:30.946498: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1110]      Summary of in-use Chunks by size: \n",
      "2025-12-12 11:55:30.946502: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 64 Chunks of size 256 totalling 16.0KiB\n",
      "2025-12-12 11:55:30.946505: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 11 Chunks of size 512 totalling 5.5KiB\n",
      "2025-12-12 11:55:30.946507: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 768 totalling 1.5KiB\n",
      "2025-12-12 11:55:30.946510: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 4 Chunks of size 1024 totalling 4.0KiB\n",
      "2025-12-12 11:55:30.946512: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 1280 totalling 2.5KiB\n",
      "2025-12-12 11:55:30.946515: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 1536 totalling 1.5KiB\n",
      "2025-12-12 11:55:30.946517: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 1792 totalling 3.5KiB\n",
      "2025-12-12 11:55:30.946520: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 3 Chunks of size 3584 totalling 10.5KiB\n",
      "2025-12-12 11:55:30.946522: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 8960 totalling 17.5KiB\n",
      "2025-12-12 11:55:30.946525: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 65536 totalling 128.0KiB\n",
      "2025-12-12 11:55:30.946527: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 129536 totalling 126.5KiB\n",
      "2025-12-12 11:55:30.946530: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 131072 totalling 256.0KiB\n",
      "2025-12-12 11:55:30.946532: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 180224 totalling 176.0KiB\n",
      "2025-12-12 11:55:30.946535: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 221184 totalling 432.0KiB\n",
      "2025-12-12 11:55:30.946538: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 298752 totalling 291.8KiB\n",
      "2025-12-12 11:55:30.946540: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 3 Chunks of size 884736 totalling 2.53MiB\n",
      "2025-12-12 11:55:30.946542: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 2809856 totalling 2.68MiB\n",
      "2025-12-12 11:55:30.946545: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 16771328 totalling 31.99MiB\n",
      "2025-12-12 11:55:30.946548: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 33542656 totalling 63.98MiB\n",
      "2025-12-12 11:55:30.946550: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 218108672 totalling 208.00MiB\n",
      "2025-12-12 11:55:30.946553: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 268435456 totalling 256.00MiB\n",
      "2025-12-12 11:55:30.946555: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] Sum Total of in-use chunks: 566.62MiB\n",
      "2025-12-12 11:55:30.946558: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1119] Total bytes in pool: 1004666880 memory_limit_: 1004666880 available bytes: 0 curr_region_allocation_bytes_: 1073741824\n",
      "2025-12-12 11:55:30.946563: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1124] Stats: \n",
      "Limit:                      1004666880\n",
      "InUse:                       594144256\n",
      "MaxInUse:                   1003575040\n",
      "NumAllocs:                        7123\n",
      "MaxAllocSize:                339376640\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-12-12 11:55:30.946567: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:512] ******______*_________________________________**************************x********************xxxxxxx\n",
      "2025-12-12 11:55:30.946645: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 364637240 bytes.\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      " [tf-allocator-allocation-error='']\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_66763/3421830964.py\", line 2, in <module>\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 399, in fit\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 241, in function\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 154, in multi_step_on_iterator\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 125, in wrapper\n\nOut of memory while trying to allocate 364637240 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_4553]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 10: Full training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting full training session...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m history_full \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainy_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvaly_flat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     16\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(history_full\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/micromamba/envs/tfenv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_66763/3421830964.py\", line 2, in <module>\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 399, in fit\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 241, in function\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 154, in multi_step_on_iterator\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 125, in wrapper\n\nOut of memory while trying to allocate 364637240 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_4553]"
     ]
    }
   ],
   "source": [
    "# Cell 10: Full training\n",
    "print(\"Starting full training session...\")\n",
    "\n",
    "history_full = model.fit(\n",
    "    trainx_norm, trainy_flat,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(valx_norm, valy_flat),\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(history_full.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history_full.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Model Loss Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_full.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history_full.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Model Accuracy Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "axes[2].plot(history_full.history['auc'], label='Training AUC', linewidth=2)\n",
    "axes[2].plot(history_full.history['val_auc'], label='Validation AUC', linewidth=2)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('AUC')\n",
    "axes[2].set_title('Model AUC Over Time')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b120b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = keras.models.load_model('best_vessel_model.h5')\n",
    "\n",
    "test_loss, test_acc, test_auc = model.evaluate(testx_norm, testy_flat, verbose=0)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "y_pred_proba = model.predict(testx_norm, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, y_pred, \n",
    "                          target_names=['Healthy', 'Aneurysm'],\n",
    "                          digits=4))\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Healthy', 'Aneurysm'],\n",
    "            yticklabels=['Healthy', 'Aneurysm'])\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'Model (AUC = {test_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Compare to baseline\n",
    "print(\"VesselMNIST3D Baseline Performance (from MedMNIST paper):\")\n",
    "print(\"- ResNet18 (3D): AUC ~0.920, ACC ~0.890\")\n",
    "print(\"- Auto-sklearn: AUC ~0.917, ACC ~0.887\")\n",
    "print(\"\\nYour Model Performance:\")\n",
    "print(f\"- Test AUC: {test_auc:.4f}\")\n",
    "print(f\"- Test ACC: {test_acc:.4f}\")\n",
    "\n",
    "if test_auc >= 0.920:\n",
    "    print(\"\\nModel matches or exceeds the baseline!\")\n",
    "elif test_auc >= 0.900:\n",
    "    print(\"\\nGood performance! Close to the baseline.\")\n",
    "else:\n",
    "    print(\"\\nRoom for improvement\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
