{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e6f156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 GPU(s)\n",
      "  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "  GPU 1: PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n",
      "\n",
      "Attempting to use all available GPUs with memory growth enabled\n",
      "\n",
      "TensorFlow version: 2.20.0\n",
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and setup (MODIFIED - Flexible GPU selection)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.widgets import Slider\n",
    "import matplotlib\n",
    "import matplotlib.font_manager\n",
    "from medmnist import VesselMNIST3D\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        print(f\"Found {len(gpus)} GPU(s)\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"  GPU {i}: {gpu}\")\n",
    "        \n",
    "        print(\"\\nAttempting to use all available GPUs with memory growth enabled\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"No GPUs found, using CPU\")\n",
    "\n",
    "print(\"\\nTensorFlow version:\", tf.__version__)\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "117fa961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Training samples: 1335\n",
      "Validation samples: 191\n",
      "Test samples: 382\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load data\n",
    "train_dataset = VesselMNIST3D(split='train', size=28, download=True)\n",
    "trainx = []\n",
    "trainy = []\n",
    "\n",
    "test_dataset = VesselMNIST3D(split='test', size=28, download=True)\n",
    "testx = []\n",
    "testy = []\n",
    "\n",
    "val_dataset = VesselMNIST3D(split='val', size=28, download=True)\n",
    "valx = []\n",
    "valy = []\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    trainx.append(train_dataset[i][0])\n",
    "    trainy.append(train_dataset[i][1])\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    testx.append(test_dataset[i][0])\n",
    "    testy.append(test_dataset[i][1])\n",
    "\n",
    "for i in range(len(val_dataset)):\n",
    "    valx.append(val_dataset[i][0])\n",
    "    valy.append(val_dataset[i][1])\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Training samples: {len(trainx)}\")\n",
    "print(f\"Validation samples: {len(valx)}\")\n",
    "print(f\"Test samples: {len(testx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f67789bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Class 0 (Healthy): 1185, Class 1 (Aneurysm): 150\n",
      "Validation - Class 0 (Healthy): 169, Class 1 (Aneurysm): 22\n",
      "Test - Class 0 (Healthy): 339, Class 1 (Aneurysm): 43\n",
      "\n",
      "Class imbalance ratio (train): 0.127\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Analyze original class distribution\n",
    "train_labels = np.array(trainy).flatten()\n",
    "val_labels = np.array(valy).flatten()\n",
    "test_labels = np.array(testy).flatten()\n",
    "\n",
    "unique_train, counts_train = np.unique(train_labels, return_counts=True)\n",
    "unique_val, counts_val = np.unique(val_labels, return_counts=True)\n",
    "unique_test, counts_test = np.unique(test_labels, return_counts=True)\n",
    "\n",
    "print(f\"Training - Class 0 (Healthy): {counts_train[0]}, Class 1 (Aneurysm): {counts_train[1]}\")\n",
    "print(f\"Validation - Class 0 (Healthy): {counts_val[0]}, Class 1 (Aneurysm): {counts_val[1]}\")\n",
    "print(f\"Test - Class 0 (Healthy): {counts_test[0]}, Class 1 (Aneurysm): {counts_test[1]}\")\n",
    "print(f\"\\nClass imbalance ratio (train): {counts_train[1]/counts_train[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36153f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting Class 1 (Aneurysm) samples...\n",
      "Found 150 Class 1 samples\n",
      "Generating augmentations...\n",
      "  Processing sample 1/150\n",
      "  Processing sample 31/150\n",
      "  Processing sample 61/150\n",
      "  Processing sample 91/150\n",
      "  Processing sample 121/150\n",
      "Generated 900 augmented samples for Class 1\n",
      "\n",
      "Augmented Training set shapes:\n",
      "  X: (2235, 1, 28, 28, 28)\n",
      "  y: (2235,)\n",
      "\n",
      "============================================================\n",
      "Class Distribution Comparison:\n",
      "============================================================\n",
      "Original Training - Class 0: 1185, Class 1: 150\n",
      "                    Ratio: 0.127\n",
      "\n",
      "Augmented Training - Class 0: 1185, Class 1: 1050\n",
      "                     Ratio: 0.886\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Augmentation functions and execution\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate, zoom, shift\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import tensorflow as tf\n",
    "\n",
    "def augment_3d_volume(volume, num_augmentations=5):\n",
    "    augmented_volumes = []\n",
    "    \n",
    "    if hasattr(volume, 'numpy'):\n",
    "        volume = volume.numpy()\n",
    "    \n",
    "    original_shape = volume.shape\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "        aug_volume = volume.copy()\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            angle = np.random.uniform(-15, 15)\n",
    "            axes_options = [(1, 2), (1, 3), (2, 3)]\n",
    "            axes = axes_options[np.random.randint(0, len(axes_options))]\n",
    "            aug_volume = rotate(aug_volume, angle, axes=axes, reshape=False, mode='nearest')\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            scale_factor = np.random.uniform(0.9, 1.1)\n",
    "            zoom_factors = [1, scale_factor, scale_factor, scale_factor]\n",
    "            aug_volume = zoom(aug_volume, zoom_factors, mode='nearest')\n",
    "            aug_volume = resize_to_original(aug_volume, original_shape)\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            axis = np.random.randint(1, 4)\n",
    "            aug_volume = np.flip(aug_volume, axis=axis)\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            shift_amount = [0] + [np.random.randint(-3, 4) for _ in range(3)]\n",
    "            aug_volume = shift(aug_volume, shift_amount, mode='nearest')\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            noise = np.random.normal(0, 0.01, aug_volume.shape)\n",
    "            aug_volume = aug_volume + noise\n",
    "            aug_volume = np.clip(aug_volume, 0, 1)\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            brightness_factor = np.random.uniform(0.9, 1.1)\n",
    "            aug_volume = aug_volume * brightness_factor\n",
    "            aug_volume = np.clip(aug_volume, 0, 1)\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            mean = np.mean(aug_volume)\n",
    "            aug_volume = (aug_volume - mean) * np.random.uniform(0.9, 1.1) + mean\n",
    "            aug_volume = np.clip(aug_volume, 0, 1)\n",
    "        \n",
    "        if np.random.rand() > 0.7:\n",
    "            aug_volume = elastic_transform_3d(aug_volume, alpha=5, sigma=3)\n",
    "        \n",
    "        augmented_volumes.append(aug_volume)\n",
    "    \n",
    "    return augmented_volumes\n",
    "\n",
    "def elastic_transform_3d(volume, alpha=10, sigma=3):\n",
    "    shape = volume.shape[1:]\n",
    "    \n",
    "    dx = np.random.randn(*shape) * sigma\n",
    "    dy = np.random.randn(*shape) * sigma\n",
    "    dz = np.random.randn(*shape) * sigma\n",
    "    \n",
    "    dx = gaussian_filter(dx, sigma, mode='constant') * alpha\n",
    "    dy = gaussian_filter(dy, sigma, mode='constant') * alpha\n",
    "    dz = gaussian_filter(dz, sigma, mode='constant') * alpha\n",
    "    \n",
    "    z, y, x = np.meshgrid(\n",
    "        np.arange(shape[0]),\n",
    "        np.arange(shape[1]),\n",
    "        np.arange(shape[2]),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    \n",
    "    indices = [\n",
    "        np.clip(z + dz, 0, shape[0] - 1).astype(int),\n",
    "        np.clip(y + dy, 0, shape[1] - 1).astype(int),\n",
    "        np.clip(x + dx, 0, shape[2] - 1).astype(int)\n",
    "    ]\n",
    "    \n",
    "    result = np.zeros_like(volume)\n",
    "    for c in range(volume.shape[0]):\n",
    "        result[c] = volume[c][indices[0], indices[1], indices[2]]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def resize_to_original(volume, target_shape):\n",
    "    current_shape = volume.shape\n",
    "    result = volume.copy()\n",
    "    \n",
    "    for i in range(len(target_shape)):\n",
    "        if current_shape[i] > target_shape[i]:\n",
    "            diff = current_shape[i] - target_shape[i]\n",
    "            start = diff // 2\n",
    "            end = start + target_shape[i]\n",
    "            result = np.take(result, range(start, end), axis=i)\n",
    "        elif current_shape[i] < target_shape[i]:\n",
    "            diff = target_shape[i] - current_shape[i]\n",
    "            pad_before = diff // 2\n",
    "            pad_after = diff - pad_before\n",
    "            pad_width = [(0, 0)] * len(target_shape)\n",
    "            pad_width[i] = (pad_before, pad_after)\n",
    "            result = np.pad(result, pad_width, mode='edge')\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Augmenting Class 1 (Aneurysm) samples...\")\n",
    "\n",
    "augmented_trainx = []\n",
    "augmented_trainy = []\n",
    "\n",
    "trainy_flat = [label.flatten()[0] if hasattr(label, 'flatten') else label for label in trainy]\n",
    "class1_indices = [i for i, label in enumerate(trainy_flat) if label == 1]\n",
    "print(f\"Found {len(class1_indices)} Class 1 samples\")\n",
    "\n",
    "num_augmentations_per_sample = 6\n",
    "\n",
    "print(\"Generating augmentations...\")\n",
    "for i, idx in enumerate(class1_indices):\n",
    "    if i % 30 == 0:\n",
    "        print(f\"  Processing sample {i+1}/{len(class1_indices)}\")\n",
    "    \n",
    "    original_volume = trainx[idx]\n",
    "    augmented_volumes = augment_3d_volume(original_volume, num_augmentations_per_sample)\n",
    "    \n",
    "    for aug_vol in augmented_volumes:\n",
    "        augmented_trainx.append(aug_vol)\n",
    "        augmented_trainy.append(1)\n",
    "\n",
    "print(f\"Generated {len(augmented_trainx)} augmented samples for Class 1\")\n",
    "\n",
    "trainx_combined = trainx + augmented_trainx\n",
    "\n",
    "trainy_combined_flat = []\n",
    "for label in trainy:\n",
    "    if isinstance(label, np.ndarray):\n",
    "        trainy_combined_flat.append(label.flatten()[0])\n",
    "    else:\n",
    "        trainy_combined_flat.append(label)\n",
    "        \n",
    "trainy_combined_flat.extend(augmented_trainy)\n",
    "\n",
    "trainx_tensor = tf.convert_to_tensor(trainx_combined, dtype=tf.float32)\n",
    "trainy_tensor = tf.convert_to_tensor(trainy_combined_flat, dtype=tf.float32)\n",
    "testx_tensor = tf.convert_to_tensor(testx, dtype=tf.float32)\n",
    "testy_tensor = tf.convert_to_tensor(testy, dtype=tf.float32)\n",
    "valx_tensor = tf.convert_to_tensor(valx, dtype=tf.float32)\n",
    "valy_tensor = tf.convert_to_tensor(valy, dtype=tf.float32)\n",
    "\n",
    "print(f\"\\nAugmented Training set shapes:\")\n",
    "print(f\"  X: {trainx_tensor.shape}\")\n",
    "print(f\"  y: {trainy_tensor.shape}\")\n",
    "\n",
    "train_labels_aug = np.array(trainy_combined_flat).flatten()\n",
    "unique_aug, counts_aug = np.unique(train_labels_aug, return_counts=True)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Class Distribution Comparison:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Original Training - Class 0: {counts_train[0]}, Class 1: {counts_train[1]}\")\n",
    "print(f\"                    Ratio: {counts_train[1]/counts_train[0]:.3f}\")\n",
    "print(f\"\\nAugmented Training - Class 0: {counts_aug[0]}, Class 1: {counts_aug[1]}\")\n",
    "print(f\"                     Ratio: {counts_aug[1]/counts_aug[0]:.3f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de153ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02c5ef3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After transpose - Training data shape: (2235, 28, 28, 28, 1)\n",
      "Normalized training data shape: (2235, 28, 28, 28, 1)\n",
      "Training labels shape: (2235,)\n",
      "Data range: [-0.002, 0.006]\n",
      "\n",
      "Class weights to handle imbalance: {0: np.float64(0.9430379746835443), 1: np.float64(1.0642857142857143)}\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Data preprocessing\n",
    "trainx_tensor = tf.transpose(trainx_tensor, [0, 2, 3, 4, 1])\n",
    "valx_tensor = tf.transpose(valx_tensor, [0, 2, 3, 4, 1])\n",
    "testx_tensor = tf.transpose(testx_tensor, [0, 2, 3, 4, 1])\n",
    "\n",
    "print(f\"After transpose - Training data shape: {trainx_tensor.shape}\")\n",
    "\n",
    "trainx_norm = trainx_tensor / 255.0\n",
    "valx_norm = valx_tensor / 255.0\n",
    "testx_norm = testx_tensor / 255.0\n",
    "\n",
    "trainy_flat = tf.squeeze(trainy_tensor)\n",
    "valy_flat = tf.squeeze(valy_tensor)\n",
    "testy_flat = tf.squeeze(testy_tensor)\n",
    "\n",
    "print(f\"Normalized training data shape: {trainx_norm.shape}\")\n",
    "print(f\"Training labels shape: {trainy_flat.shape}\")\n",
    "print(f\"Data range: [{tf.reduce_min(trainx_norm):.3f}, {tf.reduce_max(trainx_norm):.3f}]\")\n",
    "\n",
    "train_labels_current = np.array(trainy_flat).flatten()\n",
    "unique_current, counts_current = np.unique(train_labels_current, return_counts=True)\n",
    "class_weight = {\n",
    "    0: len(train_labels_current) / (2 * counts_current[0]),\n",
    "    1: len(train_labels_current) / (2 * counts_current[1])\n",
    "}\n",
    "print(f\"\\nClass weights to handle imbalance: {class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97a0ac44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv3d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">55,360</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">221,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling3d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling3D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv3d_3 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m) │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m) │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_3 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_4 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m) │        \u001b[38;5;34m55,360\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m) │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_4 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_5 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │       \u001b[38;5;34m221,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_5 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling3d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling3D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">328,001</span> (1.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m328,001\u001b[0m (1.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">327,553</span> (1.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m327,553\u001b[0m (1.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total parameters: 328001\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Build model\n",
    "def build_3d_cnn(input_shape=(28, 28, 28, 1), dropout_rate=0.3):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        layers.Conv3D(32, kernel_size=(3, 3, 3), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Conv3D(64, kernel_size=(3, 3, 3), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Conv3D(128, kernel_size=(3, 3, 3), padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.GlobalAveragePooling3D(),\n",
    "        \n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_3d_cnn(input_shape=(28, 28, 28, 1), dropout_rate=0.3)\n",
    "model.summary()\n",
    "\n",
    "print(\"\\nTotal parameters:\", model.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4723280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled with:\n",
      "- Optimizer: Adam (lr=0.001)\n",
      "- Loss: Binary Crossentropy\n",
      "- Metrics: Accuracy, AUC\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"Model compiled with:\")\n",
    "print(\"- Optimizer: Adam (lr=0.001)\")\n",
    "print(\"- Loss: Binary Crossentropy\")\n",
    "print(\"- Metrics: Accuracy, AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50e21b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks configured:\n",
      "- Early Stopping: patience=15, monitor=val_auc\n",
      "- ReduceLROnPlateau: factor=0.5, patience=5\n",
      "- ModelCheckpoint: saves best val_auc\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Setup callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'best_vessel_model.h5',\n",
    "    monitor='val_auc',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"- Early Stopping: patience=15, monitor=val_auc\")\n",
    "print(\"- ReduceLROnPlateau: factor=0.5, patience=5\")\n",
    "print(\"- ModelCheckpoint: saves best val_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62a414ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.6935 - auc: 0.7618 - loss: 0.5852 - val_accuracy: 0.1152 - val_auc: 0.8105 - val_loss: 1.5433\n",
      "Epoch 2/2\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8089 - auc: 0.8893 - loss: 0.4063 - val_accuracy: 0.8848 - val_auc: 0.5648 - val_loss: 0.8358\n",
      "\n",
      "======================================================================\n",
      "INITIAL TEST RESULTS\n",
      "======================================================================\n",
      "Epoch 1 - Training Loss: 0.5852, Val Loss: 1.5433\n",
      "Epoch 2 - Training Loss: 0.4063, Val Loss: 0.8358\n",
      "\n",
      "Training AUC: 0.8893\n",
      "Validation AUC: 0.5648\n",
      "\n",
      "Loss is decreasing - model is learning!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Quick test (2 epochs)\n",
    "history = model.fit(\n",
    "    trainx_norm, trainy_flat,\n",
    "    batch_size=8,\n",
    "    epochs=2,\n",
    "    validation_data=(valx_norm, valy_flat),\n",
    "    class_weight=class_weight,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INITIAL TEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Epoch 1 - Training Loss: {history.history['loss'][0]:.4f}, Val Loss: {history.history['val_loss'][0]:.4f}\")\n",
    "print(f\"Epoch 2 - Training Loss: {history.history['loss'][1]:.4f}, Val Loss: {history.history['val_loss'][1]:.4f}\")\n",
    "print(f\"\\nTraining AUC: {history.history['auc'][-1]:.4f}\")\n",
    "print(f\"Validation AUC: {history.history['val_auc'][-1]:.4f}\")\n",
    "\n",
    "if history.history['loss'][1] < history.history['loss'][0]:\n",
    "    print(\"\\nLoss is decreasing - model is learning!\")\n",
    "else:\n",
    "    print(\"\\nLoss not decreasing - may need to adjust hyperparameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2a8c4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full training session...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 12:31:46.668228: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 347.75MiB (rounded to 364637440)requested by op \n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-12-12 12:31:46.668263: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1049] BFCAllocator dump for GPU_0_bfc\n",
      "2025-12-12 12:31:46.668277: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (256): \tTotal Chunks: 67, Chunks in use: 67. 16.8KiB allocated for chunks. 16.8KiB in use in bin. 6.1KiB client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668286: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (512): \tTotal Chunks: 15, Chunks in use: 15. 8.8KiB allocated for chunks. 8.8KiB in use in bin. 8.2KiB client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668293: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (1024): \tTotal Chunks: 8, Chunks in use: 8. 9.8KiB allocated for chunks. 9.8KiB in use in bin. 7.1KiB client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668302: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (2048): \tTotal Chunks: 4, Chunks in use: 4. 12.8KiB allocated for chunks. 12.8KiB in use in bin. 11.6KiB client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668310: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (4096): \tTotal Chunks: 1, Chunks in use: 0. 6.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668319: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (8192): \tTotal Chunks: 4, Chunks in use: 4. 35.0KiB allocated for chunks. 35.0KiB in use in bin. 34.9KiB client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668326: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (16384): \tTotal Chunks: 1, Chunks in use: 0. 31.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668333: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668341: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (65536): \tTotal Chunks: 3, Chunks in use: 3. 192.0KiB allocated for chunks. 192.0KiB in use in bin. 192.0KiB client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668349: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (131072): \tTotal Chunks: 6, Chunks in use: 5. 992.0KiB allocated for chunks. 864.0KiB in use in bin. 816.0KiB client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668357: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (262144): \tTotal Chunks: 1, Chunks in use: 1. 368.0KiB allocated for chunks. 368.0KiB in use in bin. 216.0KiB client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668365: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (524288): \tTotal Chunks: 4, Chunks in use: 3. 3.20MiB allocated for chunks. 2.53MiB in use in bin. 2.53MiB client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668372: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (1048576): \tTotal Chunks: 1, Chunks in use: 0. 1.12MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668379: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 1. 2.68MiB allocated for chunks. 2.68MiB in use in bin. 2.68MiB client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668386: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 0. 7.42MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668394: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (8388608): \tTotal Chunks: 3, Chunks in use: 3. 47.98MiB allocated for chunks. 47.98MiB in use in bin. 47.98MiB client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668402: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (16777216): \tTotal Chunks: 3, Chunks in use: 2. 95.97MiB allocated for chunks. 63.98MiB in use in bin. 63.98MiB client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668409: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668420: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668428: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (134217728): \tTotal Chunks: 3, Chunks in use: 2. 542.13MiB allocated for chunks. 395.16MiB in use in bin. 374.32MiB client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668436: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 1. 256.00MiB allocated for chunks. 256.00MiB in use in bin. 187.16MiB client-requested in use in bin.\n",
      "2025-12-12 12:31:46.668443: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1072] Bin for 347.75MiB was 256.00MiB, Chunk State: \n",
      "2025-12-12 12:31:46.668449: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1085] Next region of size 467795968\n",
      "2025-12-12 12:31:46.668459: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8c000000 of size 16771328 next 10\n",
      "2025-12-12 12:31:46.668465: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 711b8cffe900 of size 33542656 next 11\n",
      "2025-12-12 12:31:46.668470: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effbb00 of size 256 next 12\n",
      "2025-12-12 12:31:46.668476: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effbc00 of size 256 next 122\n",
      "2025-12-12 12:31:46.668481: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effbd00 of size 256 next 121\n",
      "2025-12-12 12:31:46.668487: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effbe00 of size 256 next 119\n",
      "2025-12-12 12:31:46.668492: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effbf00 of size 256 next 118\n",
      "2025-12-12 12:31:46.668498: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc000 of size 256 next 17\n",
      "2025-12-12 12:31:46.668503: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc100 of size 256 next 18\n",
      "2025-12-12 12:31:46.668508: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc200 of size 256 next 19\n",
      "2025-12-12 12:31:46.668514: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc300 of size 256 next 20\n",
      "2025-12-12 12:31:46.668519: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc400 of size 256 next 22\n",
      "2025-12-12 12:31:46.668524: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc500 of size 256 next 23\n",
      "2025-12-12 12:31:46.668530: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc600 of size 256 next 5\n",
      "2025-12-12 12:31:46.668535: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc700 of size 256 next 87\n",
      "2025-12-12 12:31:46.668541: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc800 of size 256 next 123\n",
      "2025-12-12 12:31:46.668546: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effc900 of size 256 next 28\n",
      "2025-12-12 12:31:46.668551: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effca00 of size 256 next 29\n",
      "2025-12-12 12:31:46.668557: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effcb00 of size 256 next 30\n",
      "2025-12-12 12:31:46.668562: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effcc00 of size 256 next 120\n",
      "2025-12-12 12:31:46.668568: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effcd00 of size 256 next 115\n",
      "2025-12-12 12:31:46.668573: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effce00 of size 768 next 37\n",
      "2025-12-12 12:31:46.668581: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effd100 of size 256 next 38\n",
      "2025-12-12 12:31:46.668586: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effd200 of size 256 next 39\n",
      "2025-12-12 12:31:46.668592: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effd300 of size 512 next 113\n",
      "2025-12-12 12:31:46.668599: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effd500 of size 512 next 112\n",
      "2025-12-12 12:31:46.668605: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effd700 of size 512 next 114\n",
      "2025-12-12 12:31:46.668610: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effd900 of size 512 next 109\n",
      "2025-12-12 12:31:46.668616: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effdb00 of size 256 next 108\n",
      "2025-12-12 12:31:46.668621: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effdc00 of size 256 next 46\n",
      "2025-12-12 12:31:46.668626: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effdd00 of size 256 next 47\n",
      "2025-12-12 12:31:46.668632: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effde00 of size 256 next 48\n",
      "2025-12-12 12:31:46.668637: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effdf00 of size 1792 next 95\n",
      "2025-12-12 12:31:46.668643: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effe600 of size 1024 next 99\n",
      "2025-12-12 12:31:46.668649: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effea00 of size 1792 next 26\n",
      "2025-12-12 12:31:46.668654: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff100 of size 256 next 49\n",
      "2025-12-12 12:31:46.668659: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff200 of size 256 next 52\n",
      "2025-12-12 12:31:46.668665: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff300 of size 256 next 50\n",
      "2025-12-12 12:31:46.668670: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff400 of size 256 next 54\n",
      "2025-12-12 12:31:46.668676: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff500 of size 256 next 55\n",
      "2025-12-12 12:31:46.668681: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff600 of size 256 next 51\n",
      "2025-12-12 12:31:46.668686: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff700 of size 256 next 58\n",
      "2025-12-12 12:31:46.668692: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff800 of size 256 next 56\n",
      "2025-12-12 12:31:46.668697: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efff900 of size 256 next 59\n",
      "2025-12-12 12:31:46.668702: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efffa00 of size 768 next 74\n",
      "2025-12-12 12:31:46.668708: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efffd00 of size 256 next 126\n",
      "2025-12-12 12:31:46.668713: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8efffe00 of size 256 next 61\n",
      "2025-12-12 12:31:46.668719: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8effff00 of size 1024 next 107\n",
      "2025-12-12 12:31:46.668724: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000300 of size 256 next 105\n",
      "2025-12-12 12:31:46.668729: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000400 of size 256 next 94\n",
      "2025-12-12 12:31:46.668735: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000500 of size 256 next 101\n",
      "2025-12-12 12:31:46.668740: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000600 of size 256 next 100\n",
      "2025-12-12 12:31:46.668746: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000700 of size 768 next 98\n",
      "2025-12-12 12:31:46.668751: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000a00 of size 256 next 36\n",
      "2025-12-12 12:31:46.668756: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000b00 of size 256 next 57\n",
      "2025-12-12 12:31:46.668762: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000c00 of size 256 next 66\n",
      "2025-12-12 12:31:46.668767: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000d00 of size 256 next 67\n",
      "2025-12-12 12:31:46.668773: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000e00 of size 256 next 68\n",
      "2025-12-12 12:31:46.668778: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f000f00 of size 8960 next 69\n",
      "2025-12-12 12:31:46.668784: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f003200 of size 256 next 70\n",
      "2025-12-12 12:31:46.668789: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f003300 of size 256 next 81\n",
      "2025-12-12 12:31:46.668795: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f003400 of size 2304 next 125\n",
      "2025-12-12 12:31:46.668800: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f003d00 of size 768 next 64\n",
      "2025-12-12 12:31:46.668806: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f004000 of size 256 next 73\n",
      "2025-12-12 12:31:46.668812: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b8f004100 of size 196250880 next 84\n",
      "2025-12-12 12:31:46.668817: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b9ab2ce00 of size 8960 next 60\n",
      "2025-12-12 12:31:46.668823: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b9ab2f100 of size 33542656 next 44\n",
      "2025-12-12 12:31:46.668829: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711b9cb2c300 of size 33542656 next 76\n",
      "2025-12-12 12:31:46.668837: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 711b9eb29500 of size 154102528 next 18446744073709551615\n",
      "2025-12-12 12:31:46.668842: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1085] Next region of size 268435456\n",
      "2025-12-12 12:31:46.668848: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711ba8000000 of size 1280 next 2\n",
      "2025-12-12 12:31:46.668854: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711ba8000500 of size 8960 next 3\n",
      "2025-12-12 12:31:46.668859: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711ba8002800 of size 16771328 next 71\n",
      "2025-12-12 12:31:46.668865: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711ba9001100 of size 16771328 next 4\n",
      "2025-12-12 12:31:46.668870: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711ba9fffa00 of size 3584 next 124\n",
      "2025-12-12 12:31:46.668878: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 711baa000800 of size 131072 next 102\n",
      "2025-12-12 12:31:46.668884: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa020800 of size 65536 next 103\n",
      "2025-12-12 12:31:46.668889: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa030800 of size 8960 next 45\n",
      "2025-12-12 12:31:46.668895: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa032b00 of size 1024 next 138\n",
      "2025-12-12 12:31:46.668901: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 711baa032f00 of size 6400 next 41\n",
      "2025-12-12 12:31:46.668906: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa034800 of size 3584 next 40\n",
      "2025-12-12 12:31:46.668912: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa035600 of size 3584 next 42\n",
      "2025-12-12 12:31:46.668917: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa036400 of size 256 next 13\n",
      "2025-12-12 12:31:46.668923: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa036500 of size 256 next 34\n",
      "2025-12-12 12:31:46.668928: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa036600 of size 256 next 32\n",
      "2025-12-12 12:31:46.668934: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa036700 of size 256 next 31\n",
      "2025-12-12 12:31:46.668939: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa036800 of size 256 next 33\n",
      "2025-12-12 12:31:46.668945: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa036900 of size 256 next 35\n",
      "2025-12-12 12:31:46.668950: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa036a00 of size 256 next 24\n",
      "2025-12-12 12:31:46.668956: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa036b00 of size 256 next 27\n",
      "2025-12-12 12:31:46.668961: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa036c00 of size 256 next 62\n",
      "2025-12-12 12:31:46.668966: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa036d00 of size 256 next 77\n",
      "2025-12-12 12:31:46.668972: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa036e00 of size 256 next 65\n",
      "2025-12-12 12:31:46.668977: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa036f00 of size 256 next 75\n",
      "2025-12-12 12:31:46.668983: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa037000 of size 512 next 85\n",
      "2025-12-12 12:31:46.668988: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa037200 of size 512 next 79\n",
      "2025-12-12 12:31:46.668994: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa037400 of size 512 next 80\n",
      "2025-12-12 12:31:46.668999: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa037600 of size 512 next 63\n",
      "2025-12-12 12:31:46.669005: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa037800 of size 512 next 72\n",
      "2025-12-12 12:31:46.669010: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa037a00 of size 512 next 86\n",
      "2025-12-12 12:31:46.669016: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa037c00 of size 1024 next 128\n",
      "2025-12-12 12:31:46.669021: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa038000 of size 1024 next 129\n",
      "2025-12-12 12:31:46.669026: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa038400 of size 256 next 132\n",
      "2025-12-12 12:31:46.669032: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa038500 of size 256 next 133\n",
      "2025-12-12 12:31:46.669037: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa038600 of size 256 next 134\n",
      "2025-12-12 12:31:46.669043: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa038700 of size 256 next 135\n",
      "2025-12-12 12:31:46.669048: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa038800 of size 256 next 136\n",
      "2025-12-12 12:31:46.669054: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa038900 of size 256 next 137\n",
      "2025-12-12 12:31:46.669059: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 711baa038a00 of size 32256 next 104\n",
      "2025-12-12 12:31:46.669065: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa040800 of size 180224 next 116\n",
      "2025-12-12 12:31:46.669071: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa06c800 of size 221184 next 117\n",
      "2025-12-12 12:31:46.669076: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 711baa0a2800 of size 1171456 next 90\n",
      "2025-12-12 12:31:46.669082: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa1c0800 of size 221184 next 21\n",
      "2025-12-12 12:31:46.669087: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa1f6800 of size 376832 next 110\n",
      "2025-12-12 12:31:46.669093: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa252800 of size 884736 next 111\n",
      "2025-12-12 12:31:46.669099: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 711baa32a800 of size 702464 next 16\n",
      "2025-12-12 12:31:46.669104: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa3d6000 of size 884736 next 82\n",
      "2025-12-12 12:31:46.669110: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa4ae000 of size 884736 next 88\n",
      "2025-12-12 12:31:46.669115: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa586000 of size 131072 next 78\n",
      "2025-12-12 12:31:46.669121: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa5a6000 of size 131072 next 127\n",
      "2025-12-12 12:31:46.669126: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa5c6000 of size 65536 next 130\n",
      "2025-12-12 12:31:46.669132: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa5d6000 of size 65536 next 131\n",
      "2025-12-12 12:31:46.669138: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baa5e6000 of size 2809856 next 97\n",
      "2025-12-12 12:31:46.669143: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 711baa894000 of size 7776512 next 6\n",
      "2025-12-12 12:31:46.669149: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baaffe900 of size 768 next 7\n",
      "2025-12-12 12:31:46.669154: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baaffec00 of size 256 next 8\n",
      "2025-12-12 12:31:46.669160: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711baaffed00 of size 218108672 next 18446744073709551615\n",
      "2025-12-12 12:31:46.669165: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1085] Next region of size 268435456\n",
      "2025-12-12 12:31:46.669184: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 711bc8000000 of size 268435456 next 18446744073709551615\n",
      "2025-12-12 12:31:46.669190: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1110]      Summary of in-use Chunks by size: \n",
      "2025-12-12 12:31:46.669196: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 67 Chunks of size 256 totalling 16.8KiB\n",
      "2025-12-12 12:31:46.669200: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 10 Chunks of size 512 totalling 5.0KiB\n",
      "2025-12-12 12:31:46.669202: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 5 Chunks of size 768 totalling 3.8KiB\n",
      "2025-12-12 12:31:46.669205: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 5 Chunks of size 1024 totalling 5.0KiB\n",
      "2025-12-12 12:31:46.669207: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2025-12-12 12:31:46.669210: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 1792 totalling 3.5KiB\n",
      "2025-12-12 12:31:46.669212: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 2304 totalling 2.2KiB\n",
      "2025-12-12 12:31:46.669215: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 3 Chunks of size 3584 totalling 10.5KiB\n",
      "2025-12-12 12:31:46.669217: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 4 Chunks of size 8960 totalling 35.0KiB\n",
      "2025-12-12 12:31:46.669220: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 3 Chunks of size 65536 totalling 192.0KiB\n",
      "2025-12-12 12:31:46.669224: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 131072 totalling 256.0KiB\n",
      "2025-12-12 12:31:46.669227: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 180224 totalling 176.0KiB\n",
      "2025-12-12 12:31:46.669229: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 221184 totalling 432.0KiB\n",
      "2025-12-12 12:31:46.669232: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 376832 totalling 368.0KiB\n",
      "2025-12-12 12:31:46.669234: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 3 Chunks of size 884736 totalling 2.53MiB\n",
      "2025-12-12 12:31:46.669237: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 2809856 totalling 2.68MiB\n",
      "2025-12-12 12:31:46.669239: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 3 Chunks of size 16771328 totalling 47.98MiB\n",
      "2025-12-12 12:31:46.669242: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 33542656 totalling 63.98MiB\n",
      "2025-12-12 12:31:46.669244: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 196250880 totalling 187.16MiB\n",
      "2025-12-12 12:31:46.669247: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 218108672 totalling 208.00MiB\n",
      "2025-12-12 12:31:46.669250: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 268435456 totalling 256.00MiB\n",
      "2025-12-12 12:31:46.669252: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] Sum Total of in-use chunks: 769.81MiB\n",
      "2025-12-12 12:31:46.669255: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1119] Total bytes in pool: 1004666880 memory_limit_: 1004666880 available bytes: 0 curr_region_allocation_bytes_: 1073741824\n",
      "2025-12-12 12:31:46.669260: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1124] Stats: \n",
      "Limit:                      1004666880\n",
      "InUse:                       807201536\n",
      "MaxInUse:                   1003575040\n",
      "NumAllocs:                       12844\n",
      "MaxAllocSize:                339376640\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-12-12 12:31:46.669264: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:512] **___***************************______________**************************x********************xxxxxxx\n",
      "2025-12-12 12:31:46.669346: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 364637240 bytes.\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      " [tf-allocator-allocation-error='']\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_66763/3421830964.py\", line 2, in <module>\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 399, in fit\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 241, in function\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 154, in multi_step_on_iterator\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 125, in wrapper\n\nOut of memory while trying to allocate 364637240 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_12866]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 10: Full training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting full training session...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m history_full \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainy_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvaly_flat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     16\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(history_full\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/micromamba/envs/tfenv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_66763/3421830964.py\", line 2, in <module>\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 399, in fit\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 241, in function\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 154, in multi_step_on_iterator\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 125, in wrapper\n\nOut of memory while trying to allocate 364637240 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_12866]"
     ]
    }
   ],
   "source": [
    "# Cell 10: Full training\n",
    "print(\"Starting full training session...\")\n",
    "\n",
    "history_full = model.fit(\n",
    "    trainx_norm, trainy_flat,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(valx_norm, valy_flat),\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(history_full.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history_full.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Model Loss Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_full.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history_full.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Model Accuracy Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "axes[2].plot(history_full.history['auc'], label='Training AUC', linewidth=2)\n",
    "axes[2].plot(history_full.history['val_auc'], label='Validation AUC', linewidth=2)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('AUC')\n",
    "axes[2].set_title('Model AUC Over Time')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93b120b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL EVALUATION ON TEST SET\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 12:32:50.660341: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 12:32:50.892939: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_170', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-12-12 12:32:51.173832: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-12-12 12:32:51.173921: W external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:848] None of the algorithms provided by cuDNN heuristics worked; trying fallback algorithms.\n",
      "2025-12-12 12:32:51.173925: W external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:851] Conv: %cudnn-conv-bias-activation.9 = (f32[32,32,28,28,28]{4,3,2,1,0}, u8[0]{0}) custom-call(%bitcast.60, %bitcast.62, %arg3.4), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv3D\" op_name=\"sequential_3_1/conv3d_9_1/convolution\" source_file=\"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1221}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n",
      "2025-12-12 12:32:51.178859: W tensorflow/core/framework/op_kernel.cc:1855] OP_REQUIRES failed at xla_ops.cc:590 : UNKNOWN: Failed to determine best cudnn convolution algorithm for:\n",
      "%cudnn-conv-bias-activation.9 = (f32[32,32,28,28,28]{4,3,2,1,0}, u8[0]{0}) custom-call(%bitcast.60, %bitcast.62, %arg3.4), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv3D\" op_name=\"sequential_3_1/conv3d_9_1/convolution\" source_file=\"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1221}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n",
      "\n",
      "Original error: INTERNAL: All algorithms tried for (f32[32,32,28,28,28]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[32,1,28,28,28]{4,3,2,1,0}, f32[32,1,3,3,3]{4,3,2,1,0}, f32[32]{0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"reification_cost\":[],\"wait_on_operation_queues\":[]} failed. Falling back to default algorithm.  Per-algorithm errors:\n",
      "  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n",
      "  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n",
      "  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 109505936 bytes. [tf-allocator-allocation-error='']\n",
      "  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 109505936 bytes. [tf-allocator-allocation-error='']\n",
      "  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n",
      "  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n",
      "  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n",
      "  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 28030480 bytes. [tf-allocator-allocation-error='']\n",
      "  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 93454336 bytes. [tf-allocator-allocation-error='']\n",
      "  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n",
      "  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n",
      "\n",
      "To ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_66763/50790064.py\", line 8, in <module>\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 511, in evaluate\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 241, in function\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 154, in multi_step_on_iterator\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 125, in wrapper\n\nFailed to determine best cudnn convolution algorithm for:\n%cudnn-conv-bias-activation.9 = (f32[32,32,28,28,28]{4,3,2,1,0}, u8[0]{0}) custom-call(%bitcast.60, %bitcast.62, %arg3.4), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv3D\" op_name=\"sequential_3_1/conv3d_9_1/convolution\" source_file=\"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1221}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n\nOriginal error: INTERNAL: All algorithms tried for (f32[32,32,28,28,28]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[32,1,28,28,28]{4,3,2,1,0}, f32[32,1,3,3,3]{4,3,2,1,0}, f32[32]{0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"reification_cost\":[],\"wait_on_operation_queues\":[]} failed. Falling back to default algorithm.  Per-algorithm errors:\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 109505936 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 109505936 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 28030480 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 93454336 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_17693]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_vessel_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m test_loss, test_acc, test_auc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesty_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/micromamba/envs/tfenv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_66763/50790064.py\", line 8, in <module>\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 511, in evaluate\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 241, in function\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 154, in multi_step_on_iterator\n\n  File \"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 125, in wrapper\n\nFailed to determine best cudnn convolution algorithm for:\n%cudnn-conv-bias-activation.9 = (f32[32,32,28,28,28]{4,3,2,1,0}, u8[0]{0}) custom-call(%bitcast.60, %bitcast.62, %arg3.4), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv3D\" op_name=\"sequential_3_1/conv3d_9_1/convolution\" source_file=\"/home/atonmoy27/micromamba/envs/tfenv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1221}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n\nOriginal error: INTERNAL: All algorithms tried for (f32[32,32,28,28,28]{4,3,2,1,0}, u8[0]{0}) custom-call(f32[32,1,28,28,28]{4,3,2,1,0}, f32[32,1,3,3,3]{4,3,2,1,0}, f32[32]{0}), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=bf012_oi012->bf012, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kRelu\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"reification_cost\":[],\"wait_on_operation_queues\":[]} failed. Falling back to default algorithm.  Per-algorithm errors:\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 109505936 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 109505936 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 28030480 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 93454336 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n  Scratch allocation failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 16777216 bytes. [tf-allocator-allocation-error='']\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_17693]"
     ]
    }
   ],
   "source": [
    "# Cell 11: Evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = keras.models.load_model('best_vessel_model.h5')\n",
    "\n",
    "test_loss, test_acc, test_auc = model.evaluate(testx_norm, testy_flat, verbose=0)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "y_pred_proba = model.predict(testx_norm, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, y_pred, \n",
    "                          target_names=['Healthy', 'Aneurysm'],\n",
    "                          digits=4))\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Healthy', 'Aneurysm'],\n",
    "            yticklabels=['Healthy', 'Aneurysm'])\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'Model (AUC = {test_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Compare to baseline\n",
    "print(\"VesselMNIST3D Baseline Performance (from MedMNIST paper):\")\n",
    "print(\"- ResNet18 (3D): AUC ~0.920, ACC ~0.890\")\n",
    "print(\"- Auto-sklearn: AUC ~0.917, ACC ~0.887\")\n",
    "print(\"\\nYour Model Performance:\")\n",
    "print(f\"- Test AUC: {test_auc:.4f}\")\n",
    "print(f\"- Test ACC: {test_acc:.4f}\")\n",
    "\n",
    "if test_auc >= 0.920:\n",
    "    print(\"\\nModel matches or exceeds the baseline!\")\n",
    "elif test_auc >= 0.900:\n",
    "    print(\"\\nGood performance! Close to the baseline.\")\n",
    "else:\n",
    "    print(\"\\nRoom for improvement\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
